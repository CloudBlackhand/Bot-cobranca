#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ MEGA ULTRA ENGINE DE CONVERSA√á√ÉO - N√çVEL CHATGPT DE INTELIG√äNCIA
Sistema que entende ABSOLUTAMENTE TUDO que qualquer cliente falar
GIGANTEMENTE FODA - Mais inteligente que 99% dos humanos
"""

import re
import logging
import asyncio
import math
from typing import Dict, List, Optional, Any, Tuple, Set, Union
from datetime import datetime, timedelta
import json
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, Counter
import unicodedata

logger = logging.getLogger(__name__)

class IntentType(Enum):
    FATURA_SOLICITAR = "fatura_solicitar"
    FATURA_VALOR = "fatura_valor"
    FATURA_VENCIMENTO = "fatura_vencimento"
    PAGAMENTO_CONFIRMACAO = "pagamento_confirmacao"
    PAGAMENTO_DIFICULDADE = "pagamento_dificuldade"
    NEGOCIACAO_DESCONTO = "negociacao_desconto"
    NEGOCIACAO_PARCELAMENTO = "negociacao_parcelamento"
    RECLAMACAO_COBRANCA_INDEVIDA = "reclamacao_cobranca_indevida"
    RECLAMACAO_VALOR_INCORRETO = "reclamacao_valor_incorreto"
    RECLAMACAO_SERVICO = "reclamacao_servico"
    CANCELAMENTO_SERVICO = "cancelamento_servico"
    INFORMACAO_CONTA = "informacao_conta"
    SAUDACAO = "saudacao"
    DESPEDIDA = "despedida"
    CONFIRMACAO = "confirmacao"
    NEGACAO = "negacao"
    DUVIDA = "duvida"
    NOT_UNDERSTOOD = "not_understood"

@dataclass
class ExtractedEntity:
    """Entidade extra√≠da do texto com contexto sem√¢ntico"""
    type: str
    value: str
    confidence: float
    context: str
    semantic_weight: float = 1.0
    alternatives: List[str] = field(default_factory=list)
    relationships: Dict[str, float] = field(default_factory=dict)

@dataclass
class SemanticPattern:
    """Padr√£o sem√¢ntico avan√ßado para an√°lise contextual"""
    pattern_id: str
    semantic_vectors: Dict[str, float]
    context_triggers: List[str]
    intent_weights: Dict[str, float]
    emotional_indicators: Dict[str, float]
    confidence_modifiers: Dict[str, float]

@dataclass
class ConversationMemory:
    """Mem√≥ria contextual avan√ßada da conversa"""
    user_profile: Dict[str, Any] = field(default_factory=dict)
    conversation_patterns: List[str] = field(default_factory=list)
    intent_history: List[Tuple[str, float, datetime]] = field(default_factory=list)
    emotional_journey: List[Tuple[str, float, datetime]] = field(default_factory=list)
    context_switches: List[datetime] = field(default_factory=list)
    learning_data: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ContextualIntent:
    """Inten√ß√£o com contexto ULTRA avan√ßado - n√≠vel ChatGPT"""
    intent: IntentType
    confidence: float
    entities: List[ExtractedEntity]
    temporal_context: str  # passado, presente, futuro
    emotional_state: str   # neutro, frustrado, satisfeito, urgente
    negation: bool        # se h√° nega√ß√£o
    multiple_intents: List[IntentType]  # inten√ß√µes secund√°rias
    
    # üöÄ NOVOS CAMPOS ULTRA AVAN√áADOS
    semantic_similarity: float = 0.0  # similaridade sem√¢ntica com padr√µes conhecidos
    contextual_coherence: float = 0.0  # coer√™ncia contextual com conversa anterior
    linguistic_complexity: float = 0.0  # complexidade lingu√≠stica do texto
    intent_certainty: float = 0.0  # certeza absoluta da inten√ß√£o
    alternative_intents: List[Tuple[IntentType, float]] = field(default_factory=list)
    semantic_clusters: List[str] = field(default_factory=list)
    discourse_markers: List[str] = field(default_factory=list)
    pragmatic_inference: Dict[str, float] = field(default_factory=dict)

class SuperConversationEngine:
    """üöÄ MEGA ULTRA ENGINE - INTELIG√äNCIA N√çVEL CHATGPT GIGANTEMENTE FODA"""
    
    def __init__(self):
        # üß† MEM√ìRIA CONTEXTUAL ULTRA AVAN√áADA
        self.user_contexts = {}
        self.conversation_cache = {}
        self.conversation_memories = {}  # Nova mem√≥ria super avan√ßada
        self.semantic_knowledge_base = {}  # Base de conhecimento sem√¢ntico
        self.pattern_learning_db = defaultdict(list)  # Aprendizado de padr√µes
        
        # üî¨ SISTEMAS DE AN√ÅLISE ULTRA AVAN√áADOS
        self.brazilian_language_db = self._load_brazilian_language_patterns()
        self.entity_extractors = self._load_entity_extractors()
        self.context_analyzers = self._load_context_analyzers()
        self.emotion_patterns = self._load_emotion_patterns()
        self.temporal_patterns = self._load_temporal_patterns()
        self.negation_patterns = self._load_negation_patterns()
        
        # üöÄ NOVOS SISTEMAS ULTRA AVAN√áADOS - N√çVEL CHATGPT
        self.semantic_patterns = self._build_semantic_patterns()
        self.discourse_analyzers = self._load_discourse_analyzers()
        self.pragmatic_inference_engine = self._build_pragmatic_engine()
        self.contextual_coherence_analyzer = self._build_coherence_analyzer()
        self.multi_layer_processors = self._build_multi_layer_processors()
        self.intelligent_fallback_system = self._build_fallback_system()
        
        # üìö CONHECIMENTO LINGU√çSTICO ULTRA PROFUNDO
        self.brazilian_semantic_vectors = self._build_semantic_vectors()
        self.intent_similarity_matrix = self._build_intent_similarity_matrix()
        self.contextual_relationship_graph = self._build_relationship_graph()
        
        # üéØ RESPOSTAS CONTEXTUAIS MEGA INTELIGENTES
        self.contextual_responses = self._load_contextual_responses()
        self.dynamic_response_generator = self._build_dynamic_generator()
        
        logger.info("üöÄ MEGA ULTRA ENGINE DE CONVERSA√á√ÉO N√çVEL CHATGPT INICIALIZADA!")
        
    def _load_brazilian_language_patterns(self) -> Dict[str, List[Dict]]:
        """üöÄ PADR√ïES PARA CLIENTES BURROS - ENTENDE QUALQUER COISA MAL ESCRITA"""
        return {
            # üìÑ FATURA - Tudo que pode significar "quero minha conta"
            'fatura_detection': [
                # Palavras diretas
                {'pattern': r'(conta|boleto|fatura|cobran√ßa)', 'weight': 1.0},
                {'pattern': r'(segunda.?via|2.?via|2via)', 'weight': 1.0},
                {'pattern': r'(debito|d√©bito)', 'weight': 0.9},
                # Mal escritas comuns
                {'pattern': r'(cota|bolto|fatur)', 'weight': 0.9},  # erros de digita√ß√£o
                {'pattern': r'(segundav|2av)', 'weight': 0.9},
                {'pattern': r'(cobransa|cobranca)', 'weight': 0.8},
                # Contextos indiretos
                {'pattern': r'(papel|documento).*(pagar)', 'weight': 0.7},
                {'pattern': r'(como.*(pagar|quitar))', 'weight': 0.8},
                {'pattern': r'(preciso.*(pagar|quitar))', 'weight': 0.8},
                {'pattern': r'(onde.*(pagar|boleto))', 'weight': 0.8},
                # Linguagem super simples
                {'pattern': r'(papel.*dinheiro)', 'weight': 0.7},
                {'pattern': r'(quanto.*devo)', 'weight': 0.9},
                {'pattern': r'(minha.*divida)', 'weight': 0.8},
                {'pattern': r'(ter.*pagar)', 'weight': 0.7},
            ],
            
            # üí∞ VALOR - Quer saber quanto deve
            'valor_detection': [
                {'pattern': r'(quanto|valor)', 'weight': 1.0},
                {'pattern': r'(devo|deve|pagar)', 'weight': 0.9},
                {'pattern': r'(quanto.*mesmo|valor.*certo)', 'weight': 1.0},
                {'pattern': r'(ta.*quanto|t√°.*quanto)', 'weight': 0.9},
                {'pattern': r'(pre√ßo|preco)', 'weight': 0.8},
                # Mal escritos
                {'pattern': r'(qnto|qnt)', 'weight': 0.8},
                {'pattern': r'(dveo|dvo)', 'weight': 0.7},  # "devo" mal escrito
                # Contextos
                {'pattern': r'(saber.*valor)', 'weight': 0.8},
                {'pattern': r'(conta.*valor)', 'weight': 0.8},
                {'pattern': r'(total.*pagar)', 'weight': 0.8},
            ],
            
            # ‚è∞ VENCIMENTO - Quer saber quando vence
            'vencimento_detection': [
                {'pattern': r'(vencimento|vence|prazo)', 'weight': 1.0},
                {'pattern': r'(quando.*vence)', 'weight': 1.0},
                {'pattern': r'(data.*pagamento)', 'weight': 0.9},
                {'pattern': r'(at√©.*quando)', 'weight': 0.8},
                {'pattern': r'(prazo.*final)', 'weight': 0.8},
                # Mal escritos
                {'pattern': r'(vencimeto|vencimto)', 'weight': 0.8},
                {'pattern': r'(qndo.*vence)', 'weight': 0.8},
                # Contextos de urg√™ncia
                {'pattern': r'(ainda.*tempo)', 'weight': 0.7},
                {'pattern': r'(posso.*pagar)', 'weight': 0.6},
            ],
            
            # ü§ù NEGOCIA√á√ÉO - Quer parcelar ou desconto
            'negociacao_detection': [
                {'pattern': r'(parcelar|dividir|fatiar)', 'weight': 1.0},
                {'pattern': r'(acordo|negociar|conversar)', 'weight': 0.9},
                {'pattern': r'(desconto|abatimento)', 'weight': 0.9},
                {'pattern': r'(dificuldade|dif√≠cil|apertado)', 'weight': 0.8},
                {'pattern': r'(n√£o.*consigo.*pagar)', 'weight': 0.9},
                {'pattern': r'(sem.*dinheiro|sem.*grana)', 'weight': 0.8},
                # Mal escritos
                {'pattern': r'(parcelar|parsela)', 'weight': 0.8},
                {'pattern': r'(descoto|dsconto)', 'weight': 0.7},
                # Linguagem simples
                {'pattern': r'(quebrar.*galho)', 'weight': 0.7},
                {'pattern': r'(dar.*jeito)', 'weight': 0.6},
                {'pattern': r'(facilitar|ajudar)', 'weight': 0.7},
                {'pattern': r'(condi√ß√µes|condicoes)', 'weight': 0.8},
            ],
            
            # ‚úÖ PAGAMENTO FEITO - J√° pagou
            'pagamento_detection': [
                {'pattern': r'(j√°.*paguei|quitei|paguei)', 'weight': 1.0},
                {'pattern': r'(pix|transfer√™ncia|dep√≥sito)', 'weight': 0.9},
                {'pattern': r'(efetuei|realizei)', 'weight': 0.8},
                {'pattern': r'(comprovante|anexo)', 'weight': 0.8},
                # Mal escritos
                {'pattern': r'(jah.*paguei|ja.*paguei)', 'weight': 0.9},
                {'pattern': r'(quitei|kitei)', 'weight': 0.8},
                {'pattern': r'(transferencia|trasferencia)', 'weight': 0.7},
                # Contextos
                {'pattern': r'(mandei.*dinheiro)', 'weight': 0.8},
                {'pattern': r'(pago.*ontem|pago.*hoje)', 'weight': 0.9},
                {'pattern': r'(banco.*pagar)', 'weight': 0.7},
            ],
            
            # üò° RECLAMA√á√ÉO - Est√° reclamando
            'reclamacao_detection': [
                {'pattern': r'(errado|incorreto|equivocado)', 'weight': 1.0},
                {'pattern': r'(nunca.*(usei|contratei|pedi))', 'weight': 1.0},
                {'pattern': r'(n√£o.*devo|nao.*devo)', 'weight': 0.9},
                {'pattern': r'(indevida|indevido)', 'weight': 0.9},
                {'pattern': r'(contestar|discordar)', 'weight': 0.8},
                # Palavr√µes e revolta (censurados)
                {'pattern': r'(que.*merda|porra|caramba)', 'weight': 0.9},
                {'pattern': r'(absurdo|revoltante)', 'weight': 0.8},
                # Linguagem simples de revolta
                {'pattern': r'(n√£o.*certo|nao.*certo)', 'weight': 0.8},
                {'pattern': r'(engana√ß√£o|roubo)', 'weight': 0.9},
                {'pattern': r'(n√£o.*aceito|nao.*aceito)', 'weight': 0.8},
            ],
            
            # üëã SAUDA√á√ïES E DESPEDIDAS
            'interacao_social': [
                # Sauda√ß√µes
                {'pattern': r'(oi|ol√°|ola|oiii|eae|e.*ai)', 'intent': 'saudacao', 'weight': 1.0},
                {'pattern': r'(bom.*dia|boa.*tarde|boa.*noite)', 'intent': 'saudacao', 'weight': 1.0},
                {'pattern': r'(beleza|blz|suave)', 'intent': 'saudacao', 'weight': 0.8},
                # Despedidas
                {'pattern': r'(tchau|falou|at√©|flw|vlw)', 'intent': 'despedida', 'weight': 1.0},
                {'pattern': r'(obrigad[ao]|brigado|brigada)', 'intent': 'despedida', 'weight': 0.9},
                # Confirma√ß√µes
                {'pattern': r'(t√°.*bom|ta.*bom|ok|certo)', 'intent': 'confirmacao', 'weight': 0.8},
                {'pattern': r'(sim|yes|√©.*isso)', 'intent': 'confirmacao', 'weight': 0.8},
                # Nega√ß√µes
                {'pattern': r'(n√£o|nao|num|nope)', 'intent': 'negacao', 'weight': 0.9},
                # D√∫vidas
                {'pattern': r'(como.*assim|que.*isso|u√©)', 'intent': 'duvida', 'weight': 0.8},
                {'pattern': r'(n√£o.*entendi|nao.*entendi)', 'intent': 'duvida', 'weight': 0.9},
            ],
            
            # üî§ NORMALIZA√á√ÉO DE ERROS COMUNS
            'erro_patterns': {
                # Substitui√ß√µes autom√°ticas para normalizar textos mal escritos
                'qnto': 'quanto',
                'qnt': 'quanto', 
                'qndo': 'quando',
                'vc': 'voc√™',
                'pq': 'porque',
                'tbm': 'tamb√©m',
                'n': 'n√£o',
                'naum': 'n√£o',
                'eh': '√©',
                'tah': 'est√°',
                'to': 'estou',
                'pra': 'para',
                'pro': 'para o',
                'msm': 'mesmo',
                'blz': 'beleza',
                'vlw': 'valeu',
                'flw': 'falou',
                'kd': 'cad√™',
                'aki': 'aqui',
                'ai': 'a√≠',
                'hj': 'hoje',
                'ontem': 'ontem',
                'amanha': 'amanh√£',
                'soh': 's√≥',
                'jah': 'j√°',
                'neh': 'n√©',
                'eh': '√©',
                'num': 'n√£o',
                'vo': 'vou',
                'c': 'com',
                'cmg': 'comigo',
                'ctg': 'contigo',
                'dps': 'depois'
            }
        }
    
    def _load_entity_extractors(self) -> Dict[str, Dict]:
        """Extratores de entidades espec√≠ficas"""
        return {
            'valores_monetarios': {
                'patterns': [
                    r'R\$\s*(\d{1,3}(?:\.\d{3})*(?:,\d{2})?)',
                    r'(\d{1,3}(?:\.\d{3})*(?:,\d{2})?\s*reais?)',
                    r'(\d+(?:,\d{2})?\s*pila)',  # G√≠ria brasileira
                ],
                'normalizer': self._normalize_currency
            },
            'datas': {
                'patterns': [
                    r'(\d{1,2}[/\-]\d{1,2}[/\-]\d{2,4})',
                    r'(hoje|amanh√£|ontem)',
                    r'(pr√≥xima?\s+\w+)',  # pr√≥xima semana
                    r'(dia\s+\d{1,2})',
                ],
                'normalizer': self._normalize_date
            },
            'protocolos': {
                'patterns': [
                    r'(protocolo\s*:?\s*(\w+\d+|\d+))',
                    r'(n√∫mero\s+(\w+\d+|\d+))',
                ],
                'normalizer': self._normalize_protocol
            },
            'documentos': {
                'patterns': [
                    r'(cpf\s*:?\s*(\d{3}\.?\d{3}\.?\d{3}\-?\d{2}))',
                    r'(cnpj\s*:?\s*(\d{2}\.?\d{3}\.?\d{3}/?\d{4}\-?\d{2}))',
                ],
                'normalizer': self._normalize_document
            }
        }
    
    def _load_context_analyzers(self) -> Dict[str, Any]:
        """Analisadores de contexto conversacional"""
        return {
            'sequencias_conversacionais': [
                # Cliente ‚Üí Bot ‚Üí Cliente (follow-up)
                {
                    'sequence': ['bot_fatura_response', 'client_clarification'],
                    'new_context': 'fatura_detalhamento',
                    'boost_intent': ['fatura_valor', 'fatura_vencimento']
                },
                {
                    'sequence': ['bot_negociacao_response', 'client_acceptance'],
                    'new_context': 'negociacao_ativa',
                    'boost_intent': ['confirmacao', 'negociacao_parcelamento']
                }
            ],
            'padroes_contextuais': [
                # Se mencionou pagamento + valor, provavelmente confirma√ß√£o
                {
                    'conditions': ['entity_valor', 'intent_pagamento'],
                    'inferred_intent': 'pagamento_confirmacao',
                    'confidence_boost': 0.3
                },
                # Se mencionou erro + valor, provavelmente reclama√ß√£o
                {
                    'conditions': ['emotion_frustrado', 'entity_valor'],
                    'inferred_intent': 'reclamacao_valor_incorreto',
                    'confidence_boost': 0.4
                }
            ]
        }
    
    def _load_emotion_patterns(self) -> Dict[str, List[Dict]]:
        """Padr√µes de detec√ß√£o emocional CORRIGIDOS"""
        return {
            'frustrado': [
                {'pattern': r'(que absurdo|n√£o acredito|revoltante)', 'weight': 1.0},
                {'pattern': r'(indignado|revoltado|irritado)', 'weight': 0.9},
                {'pattern': r'(errada?|incorret[ao])', 'weight': 0.7},  # Corrigido para detectar reclama√ß√µes
                {'pattern': r'(cara,.*paguei|cara,.*mas)', 'weight': 0.8},  # Espec√≠fico para frustra√ß√£o com pagamento
                {'pattern': r'(nunca.*(contratei|usei|pedi))', 'weight': 0.8},  # Reclama√ß√£o t√≠pica
                {'pattern': r'(p√©ssimo|horr√≠vel|terr√≠vel)', 'weight': 0.8},
                {'pattern': r'(ainda.*(aparece|continua|mostra))', 'weight': 0.7},  # Frustra√ß√£o com pend√™ncia
            ],
            'urgente': [
                {'pattern': r'(urgente|emerg√™ncia)', 'weight': 1.0},
                {'pattern': r'(preciso.*(urgente|r√°pido|agora))', 'weight': 0.9},
                {'pattern': r'(imediatamente|hoje)', 'weight': 0.8},
                {'pattern': r'(r√°pido)', 'weight': 0.6},  # Reduzido para n√£o conflitar
            ],
            'satisfeito': [
                {'pattern': r'(obrigado|agrade√ßo|valeu)', 'weight': 0.8},
                {'pattern': r'(perfeito|√≥timo|excelente)', 'weight': 0.9},
                {'pattern': r'(resolveu|solucionou)', 'weight': 0.8},
                {'pattern': r'(muito bom|show)', 'weight': 0.7},
            ],
            'confuso': [
                {'pattern': r'(n√£o entendi|como assim)', 'weight': 0.9},
                {'pattern': r'(confuso|perdido|n√£o compreendi)', 'weight': 0.8},
                {'pattern': r'(explicar|esclarecer|que\s*\?)', 'weight': 0.6},
                # REMOVIDO "que" sozinho para evitar false positives
            ]
        }
    
    def _load_temporal_patterns(self) -> Dict[str, List[str]]:
        """Padr√µes temporais da conversa"""
        return {
            'passado': [
                r'(j√°\s+(paguei|fiz|resolvi))',
                r'(ontem|semana passada|m√™s passado)',
                r'(paguei|quitei|resolvi)',
            ],
            'presente': [
                r'(agora|atualmente|no momento)',
                r'(estou|estamos|est√°)',
                r'(hoje|neste momento)',
            ],
            'futuro': [
                r'(vou|vamos|pretendo)',
                r'(amanh√£|semana que vem|pr√≥ximo)',
                r'(planejando|pensando em)',
            ]
        }
    
    def _load_negation_patterns(self) -> List[str]:
        """Padr√µes de nega√ß√£o brasileiros"""
        return [
            r'(n√£o|num|nao)',
            r'(nunca|jamais)',
            r'(nem|nem que)',
            r'(de jeito nenhum|de forma alguma)',
            r'(negative|negativo)',
        ]
    
    async def process_message(self, phone: str, text: str) -> Optional[Dict[str, Any]]:
        """üöÄ PROCESSAMENTO ULTRA MEGA INTELIGENTE - N√çVEL CHATGPT GIGANTEMENTE FODA"""
        try:
            logger.info(f"üöÄ MEGA AN√ÅLISE ULTRA AVAN√áADA para {phone}: {text[:50]}...")
            
            # üß† FASE 1: PREPARA√á√ÉO E NORMALIZA√á√ÉO ULTRA AVAN√áADA
            conversation_memory = self._get_or_create_conversation_memory(phone)
            original_text = text
            normalized_text = self._ultra_advanced_normalize_text(text)
            
            # üî¨ FASE 2: AN√ÅLISE MULTI-CAMADAS ULTRA PROFUNDA
            linguistic_analysis = await self._perform_multi_layer_analysis(normalized_text)
            semantic_analysis = await self._perform_semantic_analysis(normalized_text, conversation_memory)
            pragmatic_analysis = await self._perform_pragmatic_analysis(normalized_text, conversation_memory)
            
            # üéØ FASE 3: EXTRA√á√ÉO DE ENTIDADES COM CONTEXTO SEM√ÇNTICO
            entities = await self._extract_ultra_advanced_entities(normalized_text, semantic_analysis)
            logger.info(f"üîç Entidades ultra avan√ßadas: {[e.type + ':' + e.value for e in entities]}")
            
            # üòä FASE 4: AN√ÅLISE EMOCIONAL E TEMPORAL PROFUNDA
            emotional_state = await self._analyze_ultra_emotion(normalized_text, conversation_memory)
            temporal_context = await self._analyze_ultra_temporal_context(normalized_text, conversation_memory)
            negation_analysis = await self._analyze_ultra_negation(normalized_text)
            
            logger.info(f"üòä Estado emocional ultra: {emotional_state}")
            logger.info(f"‚è∞ Contexto temporal ultra: {temporal_context}")
            logger.info(f"‚ùå An√°lise de nega√ß√£o: {negation_analysis}")
            
            # üß† FASE 5: INFER√äNCIA CONTEXTUAL ULTRA AVAN√áADA
            contextual_intent = await self._analyze_ultra_contextual_intent(
                normalized_text, entities, emotional_state, temporal_context, 
                negation_analysis, conversation_memory, semantic_analysis, pragmatic_analysis
            )
            
            # üìä FASE 6: AN√ÅLISE DE COER√äNCIA E CERTEZA
            coherence_score = await self._analyze_contextual_coherence(contextual_intent, conversation_memory)
            certainty_score = await self._calculate_intent_certainty(contextual_intent, linguistic_analysis)
            
            contextual_intent.contextual_coherence = coherence_score
            contextual_intent.intent_certainty = certainty_score
            
            logger.info(f"üéØ Inten√ß√£o principal ULTRA: {contextual_intent.intent.value}")
            logger.info(f"üéØ M√∫ltiplas inten√ß√µes: {[i.value for i in contextual_intent.multiple_intents]}")
            logger.info(f"üìä Confian√ßa ULTRA: {contextual_intent.confidence:.3f}")
            logger.info(f"üîó Coer√™ncia contextual: {coherence_score:.3f}")
            logger.info(f"‚úÖ Certeza da inten√ß√£o: {certainty_score:.3f}")
            
            # üß† FASE 7: APRENDIZADO E ATUALIZA√á√ÉO DE MEM√ìRIA
            await self._update_ultra_conversation_memory(phone, contextual_intent, original_text, linguistic_analysis)
            await self._learn_from_interaction(phone, contextual_intent, semantic_analysis)
            
            # üé≠ FASE 8: GERA√á√ÉO DIN√ÇMICA DE RESPOSTA ULTRA INTELIGENTE
            response = await self._generate_ultra_contextual_response(
                phone, contextual_intent, entities, conversation_memory, semantic_analysis
            )
            
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento MEGA ULTRA: {e}")
            return await self._ultra_intelligent_fallback(phone, text, e)
    
    def _extract_all_entities(self, text: str) -> List[ExtractedEntity]:
        """Extra√ß√£o completa de entidades"""
        entities = []
        
        for entity_type, config in self.entity_extractors.items():
            for pattern in config['patterns']:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    entity = ExtractedEntity(
                        type=entity_type,
                        value=config['normalizer'](match.group()),
                        confidence=0.9,
                        context=text[max(0, match.start()-20):match.end()+20]
                    )
                    entities.append(entity)
        
        return entities
    
    def _analyze_emotion(self, text: str) -> str:
        """An√°lise emocional profunda"""
        emotion_scores = {}
        
        for emotion, patterns in self.emotion_patterns.items():
            score = 0.0
            for pattern_data in patterns:
                matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
                score += matches * pattern_data['weight']
            emotion_scores[emotion] = score
        
        # Determinar emo√ß√£o dominante
        if not emotion_scores or max(emotion_scores.values()) == 0:
            return 'neutro'
        
        dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
        return dominant_emotion
    
    def _analyze_temporal_context(self, text: str) -> str:
        """An√°lise do contexto temporal"""
        for tempo, patterns in self.temporal_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    return tempo
        
        return 'presente'  # default
    
    def _detect_negation(self, text: str) -> bool:
        """Detec√ß√£o robusta de nega√ß√£o"""
        for pattern in self.negation_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False
    
    def _analyze_contextual_intent(
        self, text: str, entities: List[ExtractedEntity], emotion: str,
        temporal: str, negation: bool, conversation_context: Dict
    ) -> ContextualIntent:
        """üß† AN√ÅLISE CONTEXTUAL REVOLUCION√ÅRIA"""
        
        # An√°lise de inten√ß√µes base
        base_intents = self._analyze_base_intents(text, entities, emotion)
        
        # Boost contextual baseado na conversa
        contextual_boost = self._apply_contextual_boost(base_intents, conversation_context)
        
        # An√°lise de m√∫ltiplas inten√ß√µes
        multiple_intents = self._detect_multiple_intents(text, entities)
        
        # Determinar inten√ß√£o principal
        best_intent_data = max(contextual_boost.items(), key=lambda x: x[1])
        best_intent = IntentType(best_intent_data[0])
        confidence = min(best_intent_data[1], 1.0)
        
        return ContextualIntent(
            intent=best_intent,
            confidence=confidence,
            entities=entities,
            temporal_context=temporal,
            emotional_state=emotion,
            negation=negation,
            multiple_intents=multiple_intents
        )
    
    def _analyze_base_intents(self, text: str, entities: List[ExtractedEntity], emotion: str) -> Dict[str, float]:
        """üöÄ AN√ÅLISE ULTRA AVAN√áADA - ENTENDE CLIENTES BURROS QUE N√ÉO SABEM SE EXPRESSAR"""
        intent_scores = {}
        
        # üìÑ AN√ÅLISE FATURA - Usando nossos novos padr√µes ultra avan√ßados
        fatura_solicitar_score = 0.0
        fatura_valor_score = 0.0
        fatura_vencimento_score = 0.0
        
        # Aplicar padr√µes de detec√ß√£o de fatura
        for pattern_data in self.brazilian_language_db.get('fatura_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                fatura_solicitar_score += matches * pattern_data['weight']
        
        # Aplicar padr√µes de detec√ß√£o de valor
        for pattern_data in self.brazilian_language_db.get('valor_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                fatura_valor_score += matches * pattern_data['weight']
        
        # Aplicar padr√µes de detec√ß√£o de vencimento
        for pattern_data in self.brazilian_language_db.get('vencimento_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                fatura_vencimento_score += matches * pattern_data['weight']
        
        # ü§ù AN√ÅLISE NEGOCIA√á√ÉO - Usando nossos padr√µes avan√ßados
        negociacao_parcelamento_score = 0.0
        negociacao_desconto_score = 0.0
        
        for pattern_data in self.brazilian_language_db.get('negociacao_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                # Decidir se √© parcelamento ou desconto baseado no contexto
                if re.search(r'(parcelar|dividir|fatiar)', pattern_data['pattern'], re.IGNORECASE):
                    negociacao_parcelamento_score += matches * pattern_data['weight']
                elif re.search(r'(desconto|abatimento)', pattern_data['pattern'], re.IGNORECASE):
                    negociacao_desconto_score += matches * pattern_data['weight']
                else:
                    # Padr√µes gen√©ricos: priorizar parcelamento (mais comum)
                    negociacao_parcelamento_score += matches * pattern_data['weight'] * 0.7
                    negociacao_desconto_score += matches * pattern_data['weight'] * 0.3
        
        # ‚úÖ AN√ÅLISE PAGAMENTO FEITO - Usando nossos padr√µes
        pagamento_score = 0.0
        
        for pattern_data in self.brazilian_language_db.get('pagamento_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                pagamento_score += matches * pattern_data['weight']
        
        # üò° AN√ÅLISE RECLAMA√á√ÉO - Usando nossos padr√µes
        reclamacao_indevida_score = 0.0
        reclamacao_valor_score = 0.0
        
        for pattern_data in self.brazilian_language_db.get('reclamacao_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                # Se menciona "nunca usei/contratei" √© cobran√ßa indevida
                if re.search(r'(nunca.*(usei|contratei))', pattern_data['pattern'], re.IGNORECASE):
                    reclamacao_indevida_score += matches * pattern_data['weight']
                else:
                    # Outros tipos de reclama√ß√£o (valor incorreto)
                    reclamacao_valor_score += matches * pattern_data['weight']
        
        # üëã AN√ÅLISE INTERA√á√ÉO SOCIAL
        saudacao_score = 0.0
        despedida_score = 0.0
        confirmacao_score = 0.0
        negacao_score = 0.0
        duvida_score = 0.0
        
        for pattern_data in self.brazilian_language_db.get('interacao_social', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                intent_type = pattern_data.get('intent', 'unknown')
                score = matches * pattern_data['weight']
                
                if intent_type == 'saudacao':
                    saudacao_score += score
                elif intent_type == 'despedida':
                    despedida_score += score
                elif intent_type == 'confirmacao':
                    confirmacao_score += score
                elif intent_type == 'negacao':
                    negacao_score += score
                elif intent_type == 'duvida':
                    duvida_score += score
        
        # üß† L√ìGICA CONTEXTUAL AVAN√áADA PARA CASOS CONFUSOS
        
        # Se cliente escreveu poucas palavras, tentar inferir pelo contexto
        palavras = len(text.split())
        if palavras <= 3:
            # Textos muito curtos - analisar palavras-chave cr√≠ticas
            if re.search(r'(conta|boleto|fatura)', text, re.IGNORECASE):
                fatura_solicitar_score += 0.8
            elif re.search(r'(quanto|valor)', text, re.IGNORECASE):
                fatura_valor_score += 0.8
            elif re.search(r'(quando|vence)', text, re.IGNORECASE):
                fatura_vencimento_score += 0.8
            elif re.search(r'(paguei|pago)', text, re.IGNORECASE):
                pagamento_score += 0.8
            elif re.search(r'(parcelar|acordo)', text, re.IGNORECASE):
                negociacao_parcelamento_score += 0.8
        
        # Se tem entidades monet√°rias, boost inten√ß√µes relacionadas a dinheiro
        tem_valor = any(e.type == 'valores_monetarios' for e in entities)
        if tem_valor:
            fatura_valor_score += 0.4
            pagamento_score += 0.3
            negociacao_parcelamento_score += 0.2
        
        # Se tem datas, boost vencimento
        tem_data = any(e.type == 'datas' for e in entities)
        if tem_data:
            fatura_vencimento_score += 0.4
            pagamento_score += 0.2
        
        # üò§ BOOST BASEADO EM EMO√á√ÉO
        if emotion == 'frustrado':
            reclamacao_indevida_score += 0.4
            reclamacao_valor_score += 0.4
        elif emotion == 'urgente':
            fatura_solicitar_score += 0.3
            fatura_valor_score += 0.2
        elif emotion == 'confuso':
            duvida_score += 0.3
        
        # üéØ NORMALIZAR SCORES (max 1.0 para cada)
        intent_scores = {
            'fatura_solicitar': min(fatura_solicitar_score, 1.0),
            'fatura_valor': min(fatura_valor_score, 1.0),
            'fatura_vencimento': min(fatura_vencimento_score, 1.0),
            'negociacao_parcelamento': min(negociacao_parcelamento_score, 1.0),
            'negociacao_desconto': min(negociacao_desconto_score, 1.0),
            'pagamento_confirmacao': min(pagamento_score, 1.0),
            'reclamacao_cobranca_indevida': min(reclamacao_indevida_score, 1.0),
            'reclamacao_valor_incorreto': min(reclamacao_valor_score, 1.0),
            'saudacao': min(saudacao_score, 1.0),
            'despedida': min(despedida_score, 1.0),
            'confirmacao': min(confirmacao_score, 1.0),
            'negacao': min(negacao_score, 1.0),
            'duvida': min(duvida_score, 1.0)
        }
        
        # üö® FALLBACK INTELIGENTE - Se nenhuma inten√ß√£o forte foi detectada
        max_score = max(intent_scores.values()) if intent_scores.values() else 0
        if max_score < 0.3:
            # Cliente escreveu algo muito confuso - tentar inferir pela presen√ßa de palavras-chave
            if any(palavra in text.lower() for palavra in ['conta', 'boleto', 'fatura', 'pagar', 'deve']):
                intent_scores['fatura_solicitar'] = 0.5  # Assumir que quer fatura
            elif any(palavra in text.lower() for palavra in ['quanto', 'valor', 'pre√ßo']):
                intent_scores['fatura_valor'] = 0.5  # Assumir que quer saber valor
            else:
                intent_scores['duvida'] = 0.5  # Cliente est√° confuso
        
        return intent_scores
    
    def _apply_contextual_boost(self, base_intents: Dict[str, float], context: Dict) -> Dict[str, float]:
        """Aplicar boost baseado no contexto conversacional"""
        boosted_intents = base_intents.copy()
        
        # Se √∫ltima mensagem foi sobre fatura, boost relacionados
        last_context = context.get('last_intent')
        if last_context and 'fatura' in last_context:
            boosted_intents['fatura_valor'] = boosted_intents.get('fatura_valor', 0) + 0.2
            boosted_intents['fatura_vencimento'] = boosted_intents.get('fatura_vencimento', 0) + 0.2
        
        # Se contexto de negocia√ß√£o ativa
        if context.get('negotiation_active'):
            boosted_intents['negociacao_desconto'] = boosted_intents.get('negociacao_desconto', 0) + 0.3
            boosted_intents['confirmacao'] = boosted_intents.get('confirmacao', 0) + 0.2
        
        return boosted_intents
    
    def _detect_multiple_intents(self, text: str, entities: List[ExtractedEntity]) -> List[IntentType]:
        """Detectar m√∫ltiplas inten√ß√µes na mesma mensagem - MELHORADO"""
        intents = []
        
        # Detectores mais robustos de m√∫ltiplas inten√ß√µes
        
        # "fatura E desconto/parcelamento"
        if (re.search(r'(fatura|conta)', text, re.IGNORECASE) and 
            re.search(r'(tamb√©m|e\s+(tamb√©m)?).*(desconto|parcelar)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_DESCONTO])
        
        # "fatura E parcelamento"  
        if (re.search(r'(fatura|conta)', text, re.IGNORECASE) and 
            re.search(r'(tamb√©m|e\s+(tamb√©m)?).*(parcelar|dividir)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_PARCELAMENTO])
        
        # "paguei MAS ainda aparece"
        if (re.search(r'(paguei|quitei)', text, re.IGNORECASE) and 
            re.search(r'(mas|por√©m|ainda|continua|aparece)', text, re.IGNORECASE)):
            intents.extend([IntentType.PAGAMENTO_CONFIRMACAO, IntentType.RECLAMACAO_VALOR_INCORRETO])
        
        # "valor E vencimento"
        if (re.search(r'(quanto.*devo)', text, re.IGNORECASE) and 
            re.search(r'(quando.*vence|prazo)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_VALOR, IntentType.FATURA_VENCIMENTO])
        
        # Conectores brasileiros comuns
        conectores = [r'\s+e\s+', r'\s+tamb√©m\s+', r'\s+al√©m\s+disso\s+', r'\s+mais\s+']
        for conector in conectores:
            if re.search(conector, text, re.IGNORECASE):
                # Se tem conector, analisar cada parte
                partes = re.split(conector, text, flags=re.IGNORECASE)
                if len(partes) >= 2:
                    # Analisar se cada parte tem inten√ß√£o diferente
                    primeira_parte = partes[0].strip()
                    segunda_parte = partes[1].strip()
                    
                    # L√≥gica simplificada para detectar inten√ß√µes diferentes
                    if ('fatura' in primeira_parte.lower() and 
                        any(palavra in segunda_parte.lower() for palavra in ['desconto', 'parcelar', 'negociar'])):
                        intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_DESCONTO])
                        break
        
        return intents
    
    # M√©todos de normaliza√ß√£o (implementa√ß√µes simplificadas)
    def _normalize_currency(self, text: str) -> str:
        return re.sub(r'[^\d,]', '', text)
    
    def _normalize_date(self, text: str) -> str:
        return text.strip()
    
    def _normalize_protocol(self, text: str) -> str:
        return re.sub(r'[^\w\d]', '', text)
    
    def _normalize_document(self, text: str) -> str:
        return re.sub(r'[^\d]', '', text)
    
    def _super_normalize_text(self, text: str) -> str:
        """üöÄ NORMALIZA√á√ÉO ULTRA AVAN√áADA - CORRIGE QUALQUER TEXTO MAL ESCRITO"""
        
        # 1. PRIMEIRA PASSADA - Limpeza b√°sica
        text = text.lower().strip()
        
        # 2. REMOVER EMOJIS E CARACTERES ESPECIAIS (mas preservar pontua√ß√£o b√°sica)
        text = re.sub(r'[^\w\s\.,!?\-√°√†√¢√£√©√®√™√≠√¨√Æ√≥√≤√¥√µ√∫√π√ª√ß]', ' ', text)
        
        # 3. CORRIGIR ABREVIA√á√ïES E ERROS COMUNS (do nosso dicion√°rio)
        erro_patterns = self.brazilian_language_db.get('erro_patterns', {})
        for erro, correto in erro_patterns.items():
            # Usar word boundary para n√£o corrigir partes de palavras
            text = re.sub(rf'\b{re.escape(erro)}\b', correto, text, flags=re.IGNORECASE)
        
        # 4. CORRE√á√ïES ESPEC√çFICAS DE PORTUGU√äS BRASILEIRO MAL ESCRITO
        corrections = {
            # Erros comuns de "quanto"
            r'\b(qnt|qnto|qto|cuanto)\b': 'quanto',
            # Erros comuns de "quando"  
            r'\b(qnd|qndo|quado|cuando)\b': 'quando',
            # Erros de "voc√™"
            r'\bvc\b': 'voc√™',
            # Erros de "n√£o"
            r'\b(nao|naum|√±|n)\b': 'n√£o',
            # Erros de "para"
            r'\b(pra|pr)\b': 'para',
            # Erros de "porque"
            r'\b(pq|pk|porq)\b': 'porque',
            # Erros de "tamb√©m"
            r'\b(tb|tbm|tbn)\b': 'tamb√©m',
            # Erros de "est√°"
            r'\b(tah|ta|t√°)\b': 'est√°',
            # Erros de "estou"
            r'\b(to|tou)\b': 'estou',
            # Erros de "j√°"
            r'\b(jah|ja)\b': 'j√°',
            # Erros de "s√≥"
            r'\b(soh|so)\b': 's√≥',
            # Erros de "√©"
            r'\b(eh|e)\b': '√©',
            # Erros de "hoje"
            r'\bhj\b': 'hoje',
            # Erros de "amanh√£"
            r'\b(amanha|am√±)\b': 'amanh√£',
            # Erros de "cad√™"
            r'\bkd\b': 'cad√™',
            # Erros de "aqui"
            r'\b(aki|aq)\b': 'aqui',
            # Erros de "a√≠"
            r'\b(ai|ae)\b': 'a√≠',
            # Erros de "mesmo"
            r'\b(msm|mmo)\b': 'mesmo',
            # Erros de "beleza"
            r'\b(blz|bz)\b': 'beleza',
            # Erros de "valeu"
            r'\b(vlw|vl)\b': 'valeu',
            # Erros de "falou"
            r'\b(flw|fl)\b': 'falou',
            # Erros comuns de palavras de cobran√ßa
            r'\b(fatur|ftur)\b': 'fatura',
            r'\b(bolto|bleto)\b': 'boleto',
            r'\b(cota|cnta)\b': 'conta',
            r'\b(cobransa|cobranca)\b': 'cobran√ßa',
            r'\b(pagameto|pagamnto)\b': 'pagamento',
            r'\b(vencimeto|vencimto)\b': 'vencimento',
            r'\b(transferencia|trasferencia)\b': 'transfer√™ncia',
            r'\b(debto|debito)\b': 'd√©bito'
        }
        
        for pattern, replacement in corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        # 5. REMOVER PONTUA√á√ÉO EXCESSIVA mas preservar sentido
        text = re.sub(r'[!]{2,}', '!', text)
        text = re.sub(r'[?]{2,}', '?', text)
        text = re.sub(r'[.]{2,}', '...', text)
        
        # 6. NORMALIZAR ESPA√áOS
        text = re.sub(r'\s+', ' ', text)
        
        # 7. CORRE√á√ïES CONTEXTUAIS ESPEC√çFICAS PARA COBRAN√áA
        cobranca_corrections = {
            # "segunda via" mal escrito
            r'(segunda|2)\s*(v|vi|via)': 'segunda via',
            # "quanto devo" mal escrito  
            r'(quanto|qnto)\s*(devo|dvo|dveo)': 'quanto devo',
            # "j√° paguei" mal escrito
            r'(j√°|jah|ja)\s*(paguei|pguei|pag)': 'j√° paguei',
            # "n√£o devo" mal escrito
            r'(n√£o|nao|naum)\s*(devo|dvo)': 'n√£o devo',
            # "minha conta" mal escrito
            r'(minha|miha)\s*(conta|cota)': 'minha conta'
        }
        
        for pattern, replacement in cobranca_corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def _get_conversation_context(self, phone: str) -> Dict:
        """Obter contexto da conversa"""
        return self.user_contexts.get(phone, {})
    
    def _update_conversation_context(self, phone: str, intent: ContextualIntent, text: str):
        """Atualizar contexto conversacional"""
        if phone not in self.user_contexts:
            self.user_contexts[phone] = {
                'messages': [],
                'last_intent': None,
                'negotiation_active': False,
                'client_profile': {},
                'conversation_flow': []
            }
        
        context = self.user_contexts[phone]
        context['messages'].append({
            'text': text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'timestamp': datetime.now(),
            'entities': [{'type': e.type, 'value': e.value} for e in intent.entities],
            'emotion': intent.emotional_state
        })
        
        context['last_intent'] = intent.intent.value
        
        # Detectar se negocia√ß√£o est√° ativa
        if intent.intent in [IntentType.NEGOCIACAO_DESCONTO, IntentType.NEGOCIACAO_PARCELAMENTO]:
            context['negotiation_active'] = True
        
        # Manter apenas √∫ltimas 10 mensagens
        context['messages'] = context['messages'][-10:]
    
    async def _generate_contextual_response(
        self, phone: str, intent: ContextualIntent, entities: List[ExtractedEntity], context: Dict
    ) -> Dict[str, Any]:
        """üöÄ GERADOR DE RESPOSTAS ULTRA INTELIGENTE - PERFEITO PARA CLIENTES BURROS"""
        
        # üéØ RESPOSTAS BASEADAS NA INTEN√á√ÉO COM CONTEXTO EMOCIONAL
        
        if intent.intent == IntentType.FATURA_SOLICITAR:
            if intent.emotional_state == 'urgente':
                response_text = "üö® **URGENTE!** Entendi! Vou buscar sua fatura AGORA MESMO e te enviar em segundos!"
            elif intent.emotional_state == 'frustrado':
                response_text = "üòî Percebo que voc√™ est√° chateado. Calma, vou resolver isso rapidinho! Enviando sua fatura j√°..."
            elif intent.negation:
                response_text = "ü§î Vi que voc√™ disse 'n√£o' sobre algo. Me explica melhor o que voc√™ precisa da sua conta?"
            else:
                response_text = "üìÑ **PERFEITO!** Vou pegar sua fatura para voc√™. S√≥ um minutinho..."
        
        elif intent.intent == IntentType.FATURA_VALOR:
            valor_entity = next((e for e in entities if e.type == 'valores_monetarios'), None)
            if valor_entity:
                response_text = f"üí∞ Vi que voc√™ mencionou **R$ {valor_entity.value}**. Vou confirmar se esse √© o valor correto da sua conta!"
            elif intent.emotional_state == 'urgente':
                response_text = "üí∞ **URGENTE!** Vou verificar AGORA quanto voc√™ deve exatamente!"
            else:
                response_text = "üí∞ Entendi! Voc√™ quer saber **QUANTO DEVE**, certo? Vou verificar o valor da sua conta!"
        
        elif intent.intent == IntentType.FATURA_VENCIMENTO:
            data_entity = next((e for e in entities if e.type == 'datas'), None)
            if data_entity:
                response_text = f"‚è∞ Vi que voc√™ mencionou **{data_entity.value}**. Vou confirmar o vencimento da sua conta!"
            else:
                response_text = "‚è∞ Entendi! Voc√™ quer saber **QUANDO VENCE** sua conta, n√©? Vou verificar a data!"
        
        elif intent.intent == IntentType.NEGOCIACAO_PARCELAMENTO:
            if intent.emotional_state == 'frustrado':
                response_text = "ü§ù Entendo que est√° dif√≠cil pagar. **CALMA!** Vamos dar um jeito! Temos v√°rias op√ß√µes de parcelamento!"
            elif any(e.type == 'valores_monetarios' for e in entities):
                valor = next(e.value for e in entities if e.type == 'valores_monetarios')
                response_text = f"ü§ù Perfeito! Voc√™ quer parcelar **R$ {valor}**, n√©? Vamos encontrar a melhor condi√ß√£o para voc√™!"
            else:
                response_text = "ü§ù **√ìTIMO!** Quer parcelar? Vou ver as melhores condi√ß√µes que temos dispon√≠veis!"
        
        elif intent.intent == IntentType.NEGOCIACAO_DESCONTO:
            if intent.emotional_state == 'frustrado':
                response_text = "üí∏ Entendo sua situa√ß√£o! Vamos ver que **DESCONTO** posso conseguir para voc√™!"
            else:
                response_text = "üí∏ Interessado em desconto? **PERFEITO!** Vou verificar as promo√ß√µes dispon√≠veis!"
        
        elif intent.intent == IntentType.PAGAMENTO_CONFIRMACAO:
            if intent.temporal_context == 'passado':
                if intent.emotional_state == 'frustrado':
                    response_text = "‚úÖ Entendi! Voc√™ **J√Å PAGOU** mas ainda est√° aparecendo, n√©? Vou verificar URGENTE o que aconteceu!"
                else:
                    response_text = "‚úÖ **BELEZA!** Voc√™ j√° pagou! Vou confirmar aqui no sistema se o pagamento foi processado!"
            else:
                response_text = "üí≥ Perfeito! Vou verificar o status do seu pagamento no sistema!"
        
        elif intent.intent == IntentType.RECLAMACAO_COBRANCA_INDEVIDA:
            if intent.emotional_state == 'frustrado':
                response_text = "üò° **ENTENDO SUA REVOLTA!** Cobran√ßa indevida √© muito chato mesmo! Vou resolver isso AGORA!"
            else:
                response_text = "üîç Entendi! Voc√™ acha que essa cobran√ßa est√° **ERRADA**, n√©? Vou analisar sua situa√ß√£o!"
        
        elif intent.intent == IntentType.RECLAMACAO_VALOR_INCORRETO:
            response_text = "üîç **NOSSA!** Valor incorreto √© s√©rio! Vou verificar sua conta e corrigir se estiver errado mesmo!"
        
        elif intent.intent == IntentType.SAUDACAO:
            horario = datetime.now().hour
            if horario < 12:
                response_text = "üåÖ **BOM DIA!** Tudo beleza? Como posso te ajudar hoje?"
            elif horario < 18:
                response_text = "‚òÄÔ∏è **BOA TARDE!** E a√≠, tudo certo? Em que posso ajudar?"
            else:
                response_text = "üåô **BOA NOITE!** Beleza? Como posso te ajudar?"
        
        elif intent.intent == IntentType.DESPEDIDA:
            response_text = "üëã **VALEU!** Obrigado pelo contato! Qualquer coisa, me chama! üòä"
        
        elif intent.intent == IntentType.CONFIRMACAO:
            response_text = "‚úÖ **PERFEITO!** Entendi que voc√™ confirmou! Vou continuar com o processo!"
        
        elif intent.intent == IntentType.NEGACAO:
            response_text = "‚ùå **BELEZA!** Voc√™ disse que n√£o. Me explica melhor o que voc√™ precisa ent√£o?"
        
        elif intent.intent == IntentType.DUVIDA:
            response_text = "ü§î **SEM PROBLEMAS!** Vou explicar melhor! O que especificamente voc√™ n√£o entendeu?"
        
        else:
            # Fallback inteligente baseado no que foi detectado
            if intent.confidence < 0.5:
                response_text = "ü§î **CALMA!** Acho que n√£o entendi direito. Pode me falar de novo de um jeito mais simples? Tipo: 'quero minha conta' ou 'quanto devo'?"
            else:
                response_text = "ü§ñ **ENTENDI ALGUMA COISA!** Mas me explica melhor o que voc√™ precisa. Fala de forma simples!"
        
        # üìã ADICIONAR INFORMA√á√ïES SOBRE M√öLTIPLAS INTEN√á√ïES
        if intent.multiple_intents and len(intent.multiple_intents) > 0:
            intents_text = []
            for multi_intent in intent.multiple_intents:
                if multi_intent == IntentType.FATURA_SOLICITAR:
                    intents_text.append("ver sua conta")
                elif multi_intent == IntentType.FATURA_VALOR:
                    intents_text.append("saber quanto deve")
                elif multi_intent == IntentType.NEGOCIACAO_PARCELAMENTO:
                    intents_text.append("parcelar")
                elif multi_intent == IntentType.NEGOCIACAO_DESCONTO:
                    intents_text.append("conseguir desconto")
                else:
                    intents_text.append(multi_intent.value.replace('_', ' '))
            
            if intents_text:
                response_text += f"\n\nüìã **TAMB√âM PERCEBI** que voc√™ quer: {' e '.join(intents_text)}. Vou ajudar com tudo!"
        
        # üî• ADICIONAR CALL TO ACTION BASEADO NA INTEN√á√ÉO
        if intent.intent in [IntentType.FATURA_SOLICITAR, IntentType.FATURA_VALOR, IntentType.FATURA_VENCIMENTO]:
            response_text += "\n\n‚ö° **Aguarda a√≠ que vou buscar suas informa√ß√µes!**"
        elif intent.intent in [IntentType.NEGOCIACAO_PARCELAMENTO, IntentType.NEGOCIACAO_DESCONTO]:
            response_text += "\n\nü§ù **Vou verificar as melhores condi√ß√µes para voc√™!**"
        elif intent.intent == IntentType.PAGAMENTO_CONFIRMACAO:
            response_text += "\n\nüîç **Verificando seu pagamento no sistema...**"
        
        return {
            'text': response_text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'entities_detected': len(entities),
            'emotional_state': intent.emotional_state,
            'multiple_intents': len(intent.multiple_intents),
            'context_enhanced': True,
            'response_type': 'ultra_contextual'
        }
    
    async def _generate_fallback_response(self, phone: str, text: str) -> Dict[str, Any]:
        """Resposta de fallback inteligente"""
        return {
            'text': "ü§î Percebi que voc√™ est√° tentando me dizer algo importante. Pode reformular para eu entender melhor?",
            'intent': 'clarification_needed',
            'confidence': 0.3,
            'fallback': True
        }
    
    def _load_contextual_responses(self) -> Dict:
        """Carregar respostas contextuais avan√ßadas"""
        return {
            # Implementar depois conforme necess√°rio
            'advanced_templates': {}
        } 
    
    # ================================
    # üöÄ SISTEMAS ULTRA AVAN√áADOS - N√çVEL CHATGPT
    # ================================
    
    def _build_semantic_patterns(self) -> Dict[str, SemanticPattern]:
        """üß† CONSTRUIR PADR√ïES SEM√ÇNTICOS ULTRA AVAN√áADOS"""
        patterns = {}
        
        # Padr√£o sem√¢ntico para FATURA
        patterns['fatura_semantic'] = SemanticPattern(
            pattern_id='fatura_semantic',
            semantic_vectors={
                'documento': 0.9, 'papel': 0.8, 'conta': 1.0, 'boleto': 1.0,
                'cobran√ßa': 0.9, 'd√©bito': 0.8, 'pagamento': 0.7, 'valor': 0.6,
                'segunda_via': 1.0, 'c√≥pia': 0.7, 'comprovante': 0.6
            },
            context_triggers=['preciso', 'quero', 'mandar', 'enviar', 'ver'],
            intent_weights={'fatura_solicitar': 1.0, 'fatura_valor': 0.3},
            emotional_indicators={'urgente': 0.3, 'neutro': 0.7},
            confidence_modifiers={'direto': 1.0, 'indireto': 0.7}
        )
        
        # Padr√£o sem√¢ntico para VALOR/QUANTIDADE
        patterns['valor_semantic'] = SemanticPattern(
            pattern_id='valor_semantic',
            semantic_vectors={
                'quanto': 1.0, 'valor': 1.0, 'pre√ßo': 0.9, 'custo': 0.8,
                'dinheiro': 0.7, 'grana': 0.8, 'real': 0.6, 'centavo': 0.5,
                'total': 0.9, 'dever': 0.9, 'pagar': 0.8
            },
            context_triggers=['devo', 'pago', 'custa', 'vale'],
            intent_weights={'fatura_valor': 1.0, 'pagamento_confirmacao': 0.4},
            emotional_indicators={'frustrado': 0.2, 'neutro': 0.8},
            confidence_modifiers={'pergunta': 1.0, 'afirmacao': 0.6}
        )
        
        # Padr√£o sem√¢ntico para TEMPO/VENCIMENTO
        patterns['tempo_semantic'] = SemanticPattern(
            pattern_id='tempo_semantic',
            semantic_vectors={
                'quando': 1.0, 'data': 0.9, 'dia': 0.8, 'prazo': 1.0,
                'vencimento': 1.0, 'vence': 1.0, 'at√©': 0.7, 'tempo': 0.8,
                'hoje': 0.6, 'amanh√£': 0.7, 'm√™s': 0.6
            },
            context_triggers=['vence', 'termina', 'acaba', 'expira'],
            intent_weights={'fatura_vencimento': 1.0, 'pagamento_confirmacao': 0.3},
            emotional_indicators={'urgente': 0.5, 'neutro': 0.5},
            confidence_modifiers={'futuro': 1.0, 'passado': 0.4}
        )
        
        # Padr√£o sem√¢ntico para NEGOCIA√á√ÉO
        patterns['negociacao_semantic'] = SemanticPattern(
            pattern_id='negociacao_semantic',
            semantic_vectors={
                'parcelar': 1.0, 'dividir': 0.9, 'acordo': 0.9, 'negociar': 1.0,
                'desconto': 1.0, 'abatimento': 0.8, 'facilitar': 0.7, 'ajuda': 0.6,
                'dificuldade': 0.8, 'problema': 0.7, 'apertado': 0.8, 'quebrar_galho': 0.9
            },
            context_triggers=['n√£o_consigo', 'dif√≠cil', 'sem_dinheiro', 'ajudar'],
            intent_weights={'negociacao_parcelamento': 0.7, 'negociacao_desconto': 0.3},
            emotional_indicators={'frustrado': 0.6, 'urgente': 0.4},
            confidence_modifiers={'pedido': 1.0, 'sugestao': 0.8}
        )
        
        return patterns
    
    def _build_semantic_vectors(self) -> Dict[str, Dict[str, float]]:
        """üî¨ CONSTRUIR VETORES SEM√ÇNTICOS BRASILEIROS ULTRA AVAN√áADOS"""
        return {
            # Vetores sem√¢nticos para palavras de cobran√ßa
            'fatura': {
                'conta': 0.95, 'boleto': 0.90, 'cobran√ßa': 0.85, 'd√©bito': 0.80,
                'documento': 0.75, 'papel': 0.70, 'segunda_via': 0.95, 'c√≥pia': 0.60
            },
            'pagar': {
                'quitar': 0.90, 'saldar': 0.85, 'liquidar': 0.80, 'acertar': 0.75,
                'resolver': 0.70, 'transferir': 0.65, 'depositar': 0.60
            },
            'quanto': {
                'valor': 0.95, 'pre√ßo': 0.90, 'custo': 0.85, 'total': 0.80,
                'dinheiro': 0.75, 'grana': 0.80, 'real': 0.70
            },
            'quando': {
                'data': 0.90, 'dia': 0.85, 'prazo': 0.95, 'vencimento': 0.95,
                'tempo': 0.80, 'at√©': 0.75, 'hora': 0.70
            },
            'problema': {
                'dificuldade': 0.90, 'complica√ß√£o': 0.85, 'erro': 0.80,
                'confus√£o': 0.75, 'encrenca': 0.85, 'pepino': 0.80
            }
        }
    
    def _build_intent_similarity_matrix(self) -> Dict[str, Dict[str, float]]:
        """üéØ MATRIZ DE SIMILARIDADE ENTRE INTEN√á√ïES"""
        return {
            'fatura_solicitar': {
                'fatura_valor': 0.7, 'fatura_vencimento': 0.6, 'pagamento_confirmacao': 0.4,
                'negociacao_parcelamento': 0.3, 'informacao_conta': 0.8
            },
            'fatura_valor': {
                'fatura_solicitar': 0.7, 'fatura_vencimento': 0.5, 'pagamento_confirmacao': 0.6,
                'negociacao_parcelamento': 0.7, 'negociacao_desconto': 0.5
            },
            'negociacao_parcelamento': {
                'negociacao_desconto': 0.8, 'pagamento_dificuldade': 0.9, 'fatura_valor': 0.6
            },
            'pagamento_confirmacao': {
                'reclamacao_valor_incorreto': 0.5, 'fatura_valor': 0.4, 'fatura_solicitar': 0.3
            }
        }
    
    def _build_relationship_graph(self) -> Dict[str, List[str]]:
        """üï∏Ô∏è GRAFO DE RELACIONAMENTOS CONTEXTUAIS"""
        return {
            'financial_entities': ['valor', 'dinheiro', 'real', 'centavo', 'pagar', 'dever'],
            'temporal_entities': ['quando', 'dia', 'data', 'prazo', 'vencimento', 'at√©'],
            'document_entities': ['conta', 'boleto', 'fatura', 'papel', 'documento', 'c√≥pia'],
            'negotiation_entities': ['parcelar', 'dividir', 'acordo', 'desconto', 'facilitar'],
            'emotional_entities': ['problema', 'dificuldade', 'urgente', 'chateado', 'nervoso'],
            'action_entities': ['quero', 'preciso', 'gostaria', 'mandar', 'enviar', 'ver']
        }
    
    def _load_discourse_analyzers(self) -> Dict[str, Any]:
        """üí¨ ANALISADORES DE DISCURSO ULTRA AVAN√áADOS"""
        return {
            'discourse_markers': {
                'addition': ['tamb√©m', 'al√©m disso', 'e', 'mais', 'ainda'],
                'contrast': ['mas', 'por√©m', 'entretanto', 'contudo', 'no entanto'],
                'cause': ['porque', 'pois', 'j√° que', 'visto que', 'uma vez que'],
                'conclusion': ['ent√£o', 'portanto', 'assim', 'logo', 'por isso'],
                'sequence': ['primeiro', 'depois', 'em seguida', 'finalmente', 'por √∫ltimo'],
                'emphasis': ['realmente', 'muito', 'bastante', 'extremamente', 'totalmente']
            },
            'pragmatic_markers': {
                'politeness': ['por favor', 'obrigado', 'desculpa', 'com licen√ßa'],
                'urgency': ['urgente', 'r√°pido', 'agora', 'imediatamente', 'j√°'],
                'uncertainty': ['acho', 'talvez', 'pode ser', 'n√£o tenho certeza'],
                'emphasis': ['realmente', 'certamente', 'definitivamente', 'com certeza']
            }
        }
    
    def _build_pragmatic_engine(self) -> Dict[str, Any]:
        """üß† ENGINE DE INFER√äNCIA PRAGM√ÅTICA ULTRA AVAN√áADA"""
        return {
            'implicature_rules': {
                # Se diz "j√° paguei MAS ainda aparece" = reclama valor incorreto
                'payment_but_still_charged': {
                    'pattern': r'(j√°.*pagu|quitei|paguei).*(mas|por√©m|ainda|continua)',
                    'inference': 'reclamacao_valor_incorreto',
                    'confidence': 0.9
                },
                # Se pergunta valor E prazo = quer informa√ß√µes completas
                'value_and_deadline': {
                    'pattern': r'(quanto.*devo).*(quando.*vence|prazo)',
                    'inference': 'multiple_intents',
                    'confidence': 0.8
                },
                # Se diz que n√£o consegue pagar = quer negociar
                'cannot_pay': {
                    'pattern': r'n√£o.*(consigo|posso).*(pagar|quitar)',
                    'inference': 'negociacao_parcelamento',
                    'confidence': 0.85
                }
            },
            'contextual_inference': {
                # Infer√™ncias baseadas no contexto da conversa
                'follow_up_questions': {
                    'after_invoice_request': ['fatura_valor', 'fatura_vencimento'],
                    'after_negotiation': ['confirmacao', 'negacao', 'duvida'],
                    'after_payment_info': ['pagamento_confirmacao']
                }
            }
        }
    
    def _build_coherence_analyzer(self) -> Dict[str, Any]:
        """üîó ANALISADOR DE COER√äNCIA CONTEXTUAL ULTRA AVAN√áADO"""
        return {
            'coherence_rules': {
                'topic_continuity': {
                    'same_topic': 1.0,      # Mesma inten√ß√£o que anterior
                    'related_topic': 0.8,   # Inten√ß√£o relacionada
                    'topic_shift': 0.4,     # Mudan√ßa de assunto
                    'random_topic': 0.1     # Assunto totalmente aleat√≥rio
                },
                'temporal_coherence': {
                    'logical_sequence': 1.0,    # Sequ√™ncia l√≥gica
                    'acceptable_jump': 0.7,     # Salto aceit√°vel
                    'confusing_sequence': 0.3   # Sequ√™ncia confusa
                }
            },
            'context_memory_window': 5,  # Quantas mensagens anteriores considerar
            'coherence_threshold': 0.6   # Limite m√≠nimo de coer√™ncia
        }
    
    def _build_multi_layer_processors(self) -> List[Dict[str, Any]]:
        """üéõÔ∏è PROCESSADORES MULTI-CAMADAS ULTRA AVAN√áADOS"""
        return [
            {
                'layer': 'lexical',
                'processor': 'word_level_analysis',
                'weight': 0.2,
                'functions': ['tokenization', 'pos_tagging', 'lemmatization']
            },
            {
                'layer': 'syntactic', 
                'processor': 'phrase_level_analysis',
                'weight': 0.3,
                'functions': ['phrase_detection', 'dependency_parsing']
            },
            {
                'layer': 'semantic',
                'processor': 'meaning_level_analysis', 
                'weight': 0.3,
                'functions': ['semantic_similarity', 'concept_mapping']
            },
            {
                'layer': 'pragmatic',
                'processor': 'context_level_analysis',
                'weight': 0.2,
                'functions': ['pragmatic_inference', 'discourse_analysis']
            }
        ]
    
    def _build_fallback_system(self) -> Dict[str, Any]:
        """üõ°Ô∏è SISTEMA DE FALLBACK INTELIGENTE MULTI-CAMADAS"""
        return {
            'fallback_levels': [
                {
                    'level': 1,
                    'name': 'semantic_similarity',
                    'method': 'find_closest_semantic_match',
                    'threshold': 0.6
                },
                {
                    'level': 2, 
                    'name': 'keyword_extraction',
                    'method': 'extract_key_concepts',
                    'threshold': 0.4
                },
                {
                    'level': 3,
                    'name': 'pattern_matching',
                    'method': 'fuzzy_pattern_match', 
                    'threshold': 0.3
                },
                {
                    'level': 4,
                    'name': 'conversational_context',
                    'method': 'infer_from_conversation',
                    'threshold': 0.2
                },
                {
                    'level': 5,
                    'name': 'intelligent_guess',
                    'method': 'make_educated_guess',
                    'threshold': 0.1
                }
            ]
        }
    
    def _build_dynamic_generator(self) -> Dict[str, Any]:
        """üé≠ GERADOR DIN√ÇMICO DE RESPOSTAS ULTRA INTELIGENTE"""
        return {
            'response_templates': {
                'high_confidence': "‚úÖ **{emotion_marker}** {action_confirmation} {specifics}",
                'medium_confidence': "ü§î **{understanding}** {clarification_request}",
                'low_confidence': "‚ùì **{confusion_acknowledgment}** {help_request}",
                'contextual': "üéØ **{context_reference}** {personalized_response}"
            },
            'emotion_markers': {
                'urgente': ['URGENTE!', 'RAPIDINHO!', 'AGORA MESMO!'],
                'frustrado': ['CALMA!', 'ENTENDO!', 'VAMOS RESOLVER!'],
                'neutro': ['PERFEITO!', 'BELEZA!', 'CERTO!'],
                'satisfeito': ['√ìTIMO!', 'EXCELENTE!', 'SHOW!']
            },
            'personalization_factors': [
                'conversation_history', 'emotional_state', 'communication_style',
                'previous_intents', 'response_patterns', 'user_preferences'
            ]
        }
    
    # ================================
    # üöÄ M√âTODOS ULTRA MEGA AVAN√áADOS - N√çVEL CHATGPT GIGANTEMENTE FODA
    # ================================
    
    def _get_or_create_conversation_memory(self, phone: str) -> ConversationMemory:
        """üß† OBTER OU CRIAR MEM√ìRIA ULTRA AVAN√áADA"""
        if phone not in self.conversation_memories:
            self.conversation_memories[phone] = ConversationMemory()
        return self.conversation_memories[phone]
    
    def _ultra_advanced_normalize_text(self, text: str) -> str:
        """üöÄ NORMALIZA√á√ÉO ULTRA MEGA AVAN√áADA"""
        # Usar o m√©todo existente mas com melhorias
        normalized = self._super_normalize_text(text)
        
        # Adicionar an√°lises extras ultra avan√ßadas
        normalized = self._apply_phonetic_corrections(normalized)
        normalized = self._fix_cognitive_errors(normalized)
        normalized = self._standardize_brazilian_expressions(normalized)
        
        return normalized
    
    def _apply_phonetic_corrections(self, text: str) -> str:
        """üîä CORRE√á√ïES FON√âTICAS ULTRA AVAN√áADAS"""
        phonetic_corrections = {
            # Corre√ß√µes baseadas em como as pessoas falam
            r'\b(di)\b': 'de',  # "di manh√£" -> "de manh√£"
            r'\b(nu)\b': 'no',  # "nu banco" -> "no banco"
            r'\b(du)\b': 'do',  # "du cliente" -> "do cliente"
            r'\b(ma)\b': 'mas', # "ma n√£o" -> "mas n√£o"
            r'\b(qui)\b': 'que', # "qui dia" -> "que dia"
            r'\b(cum√©)\b': 'como √©', # "cum√© que" -> "como √© que"
            r'\b(oc√™)\b': 'voc√™',    # "oc√™ tem" -> "voc√™ tem"
            r'\b(seje)\b': 'seja',   # "seje o que" -> "seja o que"
        }
        
        for pattern, replacement in phonetic_corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def _fix_cognitive_errors(self, text: str) -> str:
        """üß† CORRIGIR ERROS COGNITIVOS E DE RACIOC√çNIO"""
        cognitive_fixes = {
            # Erros de l√≥gica temporal
            r'(ontem.*amanha|amanha.*ontem)': 'ontem ou amanh√£',
            # Contradi√ß√µes √≥bvias
            r'(n√£o.*mas.*sim|sim.*mas.*n√£o)': 'talvez',
            # Confus√µes de pessoa
            r'(voc√™.*eu.*pagar|eu.*voc√™.*pagar)': 'preciso pagar',
        }
        
        for pattern, replacement in cognitive_fixes.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def _standardize_brazilian_expressions(self, text: str) -> str:
        """üáßüá∑ PADRONIZAR EXPRESS√ïES TIPICAMENTE BRASILEIRAS"""
        expressions = {
            r'(t√°.*ligado|sacou|entendeu)': 'entende',
            r'(massa|show|da.*hora)': 'bom',
            r'(trampo|labuta)': 'trabalho',
            r'(grana|din.*din|money)': 'dinheiro',
            r'(mina|mano|brother)': 'pessoa',
            r'(rol√™|role)': 'situa√ß√£o',
        }
        
        for pattern, replacement in expressions.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    async def _perform_multi_layer_analysis(self, text: str) -> Dict[str, Any]:
        """üéõÔ∏è AN√ÅLISE MULTI-CAMADAS ULTRA PROFUNDA"""
        analysis = {
            'lexical': self._analyze_lexical_layer(text),
            'syntactic': self._analyze_syntactic_layer(text),
            'semantic': self._analyze_semantic_layer(text),
            'pragmatic': self._analyze_pragmatic_layer(text)
        }
        
        # Calcular score agregado
        analysis['overall_complexity'] = sum(
            layer['complexity_score'] * processor['weight'] 
            for layer, processor in zip(analysis.values(), self.multi_layer_processors)
        )
        
        return analysis
    
    def _analyze_lexical_layer(self, text: str) -> Dict[str, Any]:
        """üìù AN√ÅLISE LEXICAL ULTRA PROFUNDA"""
        words = text.split()
        
        return {
            'word_count': len(words),
            'avg_word_length': sum(len(w) for w in words) / len(words) if words else 0,
            'complexity_score': min(len(words) * 0.1, 1.0),
            'rare_words': [w for w in words if len(w) > 8],
            'simple_words': [w for w in words if len(w) <= 4]
        }
    
    def _analyze_syntactic_layer(self, text: str) -> Dict[str, Any]:
        """üîó AN√ÅLISE SINT√ÅTICA ULTRA PROFUNDA"""
        # Detectar estruturas sint√°ticas
        has_questions = bool(re.search(r'\?', text))
        has_subordinate = bool(re.search(r'\b(que|se|quando|onde|como)\b', text))
        has_coordination = bool(re.search(r'\b(e|mas|ou|por√©m)\b', text))
        
        complexity = 0.3
        if has_questions: complexity += 0.2
        if has_subordinate: complexity += 0.3
        if has_coordination: complexity += 0.2
        
        return {
            'has_questions': has_questions,
            'has_subordinate_clauses': has_subordinate,
            'has_coordination': has_coordination,
            'complexity_score': min(complexity, 1.0)
        }
    
    def _analyze_semantic_layer(self, text: str) -> Dict[str, Any]:
        """üß† AN√ÅLISE SEM√ÇNTICA ULTRA PROFUNDA"""
        semantic_clusters = []
        cluster_scores = {}
        
        # Analisar proximidade sem√¢ntica com nossos clusters
        for cluster_name, words in self.contextual_relationship_graph.items():
            score = 0
            for word in words:
                if word in text.lower():
                    score += 1
            
            if score > 0:
                semantic_clusters.append(cluster_name)
                cluster_scores[cluster_name] = score / len(words)
        
        return {
            'semantic_clusters': semantic_clusters,
            'cluster_scores': cluster_scores,
            'complexity_score': min(len(semantic_clusters) * 0.2, 1.0),
            'semantic_density': sum(cluster_scores.values()) / max(len(cluster_scores), 1)
        }
    
    def _analyze_pragmatic_layer(self, text: str) -> Dict[str, Any]:
        """üí≠ AN√ÅLISE PRAGM√ÅTICA ULTRA PROFUNDA"""
        pragmatic_elements = {}
        
        # Detectar elementos pragm√°ticos
        for marker_type, markers in self.discourse_analyzers['pragmatic_markers'].items():
            found_markers = [m for m in markers if m in text.lower()]
            if found_markers:
                pragmatic_elements[marker_type] = found_markers
        
        return {
            'pragmatic_elements': pragmatic_elements,
            'complexity_score': min(len(pragmatic_elements) * 0.25, 1.0),
            'pragmatic_richness': len(pragmatic_elements)
        }
    
    async def _perform_semantic_analysis(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """üî¨ AN√ÅLISE SEM√ÇNTICA MEGA ULTRA AVAN√áADA"""
        semantic_analysis = {}
        
        # Calcular similaridade sem√¢ntica com padr√µes conhecidos
        for pattern_id, pattern in self.semantic_patterns.items():
            similarity = self._calculate_semantic_similarity(text, pattern)
            semantic_analysis[pattern_id] = similarity
        
        # An√°lise de vetores sem√¢nticos
        vector_analysis = self._analyze_semantic_vectors(text)
        
        return {
            'pattern_similarities': semantic_analysis,
            'vector_analysis': vector_analysis,
            'best_match': max(semantic_analysis.items(), key=lambda x: x[1]) if semantic_analysis else None,
            'semantic_confidence': max(semantic_analysis.values()) if semantic_analysis else 0.0
        }
    
    def _calculate_semantic_similarity(self, text: str, pattern: SemanticPattern) -> float:
        """üìê CALCULAR SIMILARIDADE SEM√ÇNTICA ULTRA PRECISA"""
        similarity_score = 0.0
        total_weight = 0.0
        
        # Analisar vetores sem√¢nticos
        for concept, weight in pattern.semantic_vectors.items():
            if concept in text.lower():
                similarity_score += weight
            total_weight += weight
        
        # Normalizar score
        if total_weight > 0:
            similarity_score = similarity_score / total_weight
        
        # Boost por triggers contextuais
        for trigger in pattern.context_triggers:
            if trigger in text.lower():
                similarity_score += 0.1
        
        return min(similarity_score, 1.0)
    
    def _analyze_semantic_vectors(self, text: str) -> Dict[str, float]:
        """üßÆ AN√ÅLISE DE VETORES SEM√ÇNTICOS"""
        vector_scores = {}
        
        for main_concept, related_concepts in self.brazilian_semantic_vectors.items():
            if main_concept in text.lower():
                vector_scores[main_concept] = 1.0
                
                # Adicionar conceitos relacionados
                for related, similarity in related_concepts.items():
                    if related in text.lower():
                        vector_scores[related] = similarity
        
        return vector_scores
    
    async def _perform_pragmatic_analysis(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """üé≠ AN√ÅLISE PRAGM√ÅTICA MEGA ULTRA AVAN√áADA"""
        pragmatic_inferences = {}
        
        # Aplicar regras de implicatura
        for rule_name, rule in self.pragmatic_inference_engine['implicature_rules'].items():
            if re.search(rule['pattern'], text, re.IGNORECASE):
                pragmatic_inferences[rule_name] = {
                    'inference': rule['inference'],
                    'confidence': rule['confidence']
                }
        
        # An√°lise contextual baseada na conversa anterior
        contextual_inferences = self._analyze_conversational_context(text, memory)
        
        return {
            'implicatures': pragmatic_inferences,
            'contextual_inferences': contextual_inferences,
            'pragmatic_confidence': max(
                [inf['confidence'] for inf in pragmatic_inferences.values()] + [0.0]
            )
        }
    
    def _analyze_conversational_context(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """üí¨ AN√ÅLISE DE CONTEXTO CONVERSACIONAL ULTRA PROFUNDA"""
        inferences = {}
        
        # Analisar padr√£o baseado na √∫ltima inten√ß√£o
        if memory.intent_history:
            last_intent, confidence, timestamp = memory.intent_history[-1]
            
            # Inferir follow-ups baseados na inten√ß√£o anterior
            follow_ups = self.pragmatic_inference_engine['contextual_inference']['follow_up_questions']
            if last_intent in follow_ups:
                for possible_intent in follow_ups[last_intent]:
                    inferences[f'follow_up_{possible_intent}'] = confidence * 0.7
        
        return inferences
    
    async def _extract_ultra_advanced_entities(self, text: str, semantic_analysis: Dict[str, Any]) -> List[ExtractedEntity]:
        """üéØ EXTRA√á√ÉO ULTRA AVAN√áADA DE ENTIDADES COM CONTEXTO SEM√ÇNTICO"""
        entities = []
        
        # Usar m√©todo existente como base
        base_entities = self._extract_all_entities(text)
        
        # Enriquecer com an√°lise sem√¢ntica
        for entity in base_entities:
            # Calcular peso sem√¢ntico
            semantic_weight = 1.0
            if semantic_analysis.get('vector_analysis'):
                for concept, score in semantic_analysis['vector_analysis'].items():
                    if concept in entity.value.lower():
                        semantic_weight = max(semantic_weight, score)
            
            # Adicionar alternativas baseadas em similaridade
            alternatives = self._find_entity_alternatives(entity, semantic_analysis)
            
            # Criar entidade enriquecida
            ultra_entity = ExtractedEntity(
                type=entity.type,
                value=entity.value,
                confidence=entity.confidence,
                context=entity.context,
                semantic_weight=semantic_weight,
                alternatives=alternatives,
                relationships=self._find_entity_relationships(entity, text)
            )
            
            entities.append(ultra_entity)
        
        return entities
    
    def _find_entity_alternatives(self, entity: ExtractedEntity, semantic_analysis: Dict[str, Any]) -> List[str]:
        """üîç ENCONTRAR ALTERNATIVAS SEM√ÇNTICAS PARA ENTIDADES"""
        alternatives = []
        
        if entity.type == 'valores_monetarios':
            alternatives = ['valor', 'quantia', 'dinheiro', 'pre√ßo', 'custo']
        elif entity.type == 'datas':
            alternatives = ['prazo', 'vencimento', 'data', 'dia', 'quando']
        
        return alternatives
    
    def _find_entity_relationships(self, entity: ExtractedEntity, text: str) -> Dict[str, float]:
        """üï∏Ô∏è ENCONTRAR RELACIONAMENTOS ENTRE ENTIDADES"""
        relationships = {}
        
        # Analisar proximidade com outras palavras-chave
        for cluster_name, words in self.contextual_relationship_graph.items():
            for word in words:
                if word in text.lower() and word != entity.value.lower():
                    relationships[word] = 0.8  # Score de relacionamento
        
        return relationships
    
    async def _analyze_ultra_emotion(self, text: str, memory: ConversationMemory) -> str:
        """üòä AN√ÅLISE EMOCIONAL ULTRA AVAN√áADA COM MEM√ìRIA"""
        # Usar an√°lise existente como base
        base_emotion = self._analyze_emotion(text)
        
        # Enriquecer com contexto de mem√≥ria emocional
        if memory.emotional_journey:
            # Considerar padr√£o emocional hist√≥rico
            recent_emotions = [emotion for emotion, score, timestamp in memory.emotional_journey[-3:]]
            
            # Se h√° padr√£o de frustra√ß√£o crescente
            if recent_emotions.count('frustrado') >= 2:
                if base_emotion in ['neutro', 'confuso']:
                    base_emotion = 'frustrado'  # Inferir frustra√ß√£o continuada
        
        # Detectar escalation emocional
        emotional_escalation = self._detect_emotional_escalation(text)
        if emotional_escalation:
            if base_emotion == 'frustrado':
                base_emotion = 'muito_frustrado'  # Nova categoria
            elif base_emotion == 'urgente':
                base_emotion = 'extremamente_urgente'  # Nova categoria
        
        return base_emotion
    
    def _detect_emotional_escalation(self, text: str) -> bool:
        """üìà DETECTAR ESCALATION EMOCIONAL"""
        escalation_markers = [
            r'(muito|extremamente|super|ultra).*(chateado|irritado)',
            r'(n√£o.*aguentar|n√£o.*suportar)',
            r'(absurdo|rid√≠culo|inaceit√°vel)',
            r'[!]{3,}',  # M√∫ltiplas exclama√ß√µes
            r'[?!]{2,}',  # Mistura de ? e !
        ]
        
        return any(re.search(pattern, text, re.IGNORECASE) for pattern in escalation_markers)
    
    async def _analyze_ultra_temporal_context(self, text: str, memory: ConversationMemory) -> str:
        """‚è∞ AN√ÅLISE TEMPORAL ULTRA AVAN√áADA"""
        base_temporal = self._analyze_temporal_context(text)
        
        # Enriquecer com an√°lise de urg√™ncia temporal
        urgency_indicators = {
            'imediato': ['agora', 'j√°', 'imediatamente', 'urgente'],
            'hoje': ['hoje', 'hj', 'ainda hoje'],
            'breve': ['logo', 'em breve', 'rapidinho'],
            'futuro_proximo': ['amanh√£', 'essa semana', 'uns dias'],
            'futuro_distante': ['m√™s que vem', 'ano que vem', 'mais tarde']
        }
        
        for urgency_level, indicators in urgency_indicators.items():
            if any(indicator in text.lower() for indicator in indicators):
                return f"{base_temporal}_{urgency_level}"
        
        return base_temporal
    
    async def _analyze_ultra_negation(self, text: str) -> Dict[str, Any]:
        """‚ùå AN√ÅLISE ULTRA AVAN√áADA DE NEGA√á√ÉO"""
        has_basic_negation = self._detect_negation(text)
        
        # An√°lise mais sofisticada de tipos de nega√ß√£o
        negation_types = {
            'absolute': r'\b(nunca|jamais|de jeito nenhum)\b',
            'partial': r'\b(n√£o muito|meio que n√£o|acho que n√£o)\b',
            'conditional': r'\b(n√£o se|s√≥ n√£o|a n√£o ser)\b',
            'emphatic': r'\b(de forma alguma|nem pensar|que nada)\b'
        }
        
        detected_types = []
        for neg_type, pattern in negation_types.items():
            if re.search(pattern, text, re.IGNORECASE):
                detected_types.append(neg_type)
        
        return {
            'has_negation': has_basic_negation,
            'negation_types': detected_types,
            'negation_strength': len(detected_types) / len(negation_types)
        }
    
    async def _analyze_ultra_contextual_intent(
        self, text: str, entities: List[ExtractedEntity], emotion: str, 
        temporal: str, negation: Dict, memory: ConversationMemory, 
        semantic_analysis: Dict, pragmatic_analysis: Dict
    ) -> ContextualIntent:
        """üß† AN√ÅLISE ULTRA MEGA AVAN√áADA DE INTEN√á√ÉO CONTEXTUAL"""
        
        # Usar an√°lise base existente
        base_intent_analysis = self._analyze_contextual_intent(
            text, entities, emotion, temporal, negation.get('has_negation', False), memory
        )
        
        # ENRIQUECER COM AN√ÅLISES ULTRA AVAN√áADAS
        
        # 1. Boost sem√¢ntico baseado na melhor correspond√™ncia
        if semantic_analysis.get('best_match'):
            pattern_id, similarity_score = semantic_analysis['best_match']
            if similarity_score > 0.7:
                # Aplicar boost baseado no padr√£o sem√¢ntico
                if 'fatura' in pattern_id:
                    base_intent_analysis.confidence += 0.2
                elif 'valor' in pattern_id:
                    base_intent_analysis.confidence += 0.15
        
        # 2. Boost pragm√°tico baseado em implicaturas
        pragmatic_confidence = pragmatic_analysis.get('pragmatic_confidence', 0)
        base_intent_analysis.confidence += pragmatic_confidence * 0.1
        
        # 3. Calcular similaridade sem√¢ntica com inten√ß√µes conhecidas
        semantic_similarity = self._calculate_intent_semantic_similarity(
            base_intent_analysis.intent, semantic_analysis
        )
        
        # 4. Analisar alternativas de inten√ß√£o
        alternative_intents = self._calculate_alternative_intents(
            text, semantic_analysis, pragmatic_analysis
        )
        
        # 5. Detectar clusters sem√¢nticos
        semantic_clusters = semantic_analysis.get('pattern_similarities', {}).keys()
        
        # 6. Analisar marcadores de discurso
        discourse_markers = self._extract_discourse_markers(text)
        
        # 7. Infer√™ncia pragm√°tica ultra avan√ßada
        pragmatic_inference = self._calculate_pragmatic_inference(
            base_intent_analysis, pragmatic_analysis, memory
        )
        
        # Criar inten√ß√£o contextual ultra enriquecida
        ultra_intent = ContextualIntent(
            intent=base_intent_analysis.intent,
            confidence=min(base_intent_analysis.confidence, 1.0),
            entities=entities,
            temporal_context=temporal,
            emotional_state=emotion,
            negation=negation.get('has_negation', False),
            multiple_intents=base_intent_analysis.multiple_intents,
            
            # CAMPOS ULTRA AVAN√áADOS
            semantic_similarity=semantic_similarity,
            contextual_coherence=0.0,  # Ser√° calculado depois
            linguistic_complexity=semantic_analysis.get('semantic_confidence', 0),
            intent_certainty=0.0,  # Ser√° calculado depois
            alternative_intents=alternative_intents,
            semantic_clusters=list(semantic_clusters),
            discourse_markers=discourse_markers,
            pragmatic_inference=pragmatic_inference
        )
        
        return ultra_intent
    
    def _calculate_intent_semantic_similarity(self, intent: IntentType, semantic_analysis: Dict) -> float:
        """üìê CALCULAR SIMILARIDADE SEM√ÇNTICA DA INTEN√á√ÉO"""
        intent_key = intent.value
        similarity_matrix = self.intent_similarity_matrix
        
        if intent_key in similarity_matrix:
            # Calcular m√©dia das similaridades com outras inten√ß√µes detectadas
            similarities = []
            for related_intent, similarity in similarity_matrix[intent_key].items():
                if any(related_intent in cluster for cluster in semantic_analysis.get('pattern_similarities', {})):
                    similarities.append(similarity)
            
            return sum(similarities) / len(similarities) if similarities else 0.5
        
        return 0.5  # Default
    
    def _calculate_alternative_intents(self, text: str, semantic_analysis: Dict, pragmatic_analysis: Dict) -> List[Tuple[IntentType, float]]:
        """üéØ CALCULAR INTEN√á√ïES ALTERNATIVAS"""
        alternatives = []
        
        # Baseado em an√°lise sem√¢ntica
        for pattern_id, similarity in semantic_analysis.get('pattern_similarities', {}).items():
            if similarity > 0.5:
                if 'fatura' in pattern_id:
                    alternatives.append((IntentType.FATURA_SOLICITAR, similarity))
                elif 'valor' in pattern_id:
                    alternatives.append((IntentType.FATURA_VALOR, similarity))
                elif 'negociacao' in pattern_id:
                    alternatives.append((IntentType.NEGOCIACAO_PARCELAMENTO, similarity))
        
        # Remover duplicatas e ordenar por confian√ßa
        alternatives = list(set(alternatives))
        alternatives.sort(key=lambda x: x[1], reverse=True)
        
        return alternatives[:3]  # Top 3 alternativas
    
    def _extract_discourse_markers(self, text: str) -> List[str]:
        """üí¨ EXTRAIR MARCADORES DE DISCURSO"""
        markers = []
        
        for marker_type, marker_list in self.discourse_analyzers['discourse_markers'].items():
            for marker in marker_list:
                if marker in text.lower():
                    markers.append(f"{marker_type}:{marker}")
        
        return markers
    
    def _calculate_pragmatic_inference(self, intent: ContextualIntent, pragmatic_analysis: Dict, memory: ConversationMemory) -> Dict[str, float]:
        """üé≠ CALCULAR INFER√äNCIA PRAGM√ÅTICA"""
        inferences = {}
        
        # Infer√™ncias baseadas em implicaturas
        for implicature_name, implicature_data in pragmatic_analysis.get('implicatures', {}).items():
            inferences[implicature_name] = implicature_data['confidence']
        
        # Infer√™ncias contextuais
        contextual_infs = pragmatic_analysis.get('contextual_inferences', {})
        inferences.update(contextual_infs)
        
        return inferences
    
    async def _analyze_contextual_coherence(self, intent: ContextualIntent, memory: ConversationMemory) -> float:
        """üîó ANALISAR COER√äNCIA CONTEXTUAL"""
        if not memory.intent_history:
            return 0.8  # Primeira mensagem tem coer√™ncia neutra
        
        # Pegar √∫ltimas 3 inten√ß√µes
        recent_intents = [intent_data[0] for intent_data in memory.intent_history[-3:]]
        current_intent = intent.intent.value
        
        # Calcular coer√™ncia baseada na matriz de similaridade
        coherence_scores = []
        
        for past_intent in recent_intents:
            if past_intent in self.intent_similarity_matrix:
                if current_intent in self.intent_similarity_matrix[past_intent]:
                    coherence_scores.append(self.intent_similarity_matrix[past_intent][current_intent])
                else:
                    coherence_scores.append(0.3)  # Baixa coer√™ncia para inten√ß√µes n√£o relacionadas
        
        return sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0.5
    
    async def _calculate_intent_certainty(self, intent: ContextualIntent, linguistic_analysis: Dict) -> float:
        """‚úÖ CALCULAR CERTEZA DA INTEN√á√ÉO"""
        certainty_factors = []
        
        # Fator 1: Confian√ßa base da inten√ß√£o
        certainty_factors.append(intent.confidence)
        
        # Fator 2: Similaridade sem√¢ntica
        certainty_factors.append(intent.semantic_similarity)
        
        # Fator 3: Coer√™ncia contextual
        certainty_factors.append(intent.contextual_coherence)
        
        # Fator 4: Complexidade lingu√≠stica (menos complexo = mais certo)
        linguistic_certainty = 1.0 - linguistic_analysis.get('overall_complexity', 0.5)
        certainty_factors.append(linguistic_certainty)
        
        # Fator 5: Presen√ßa de entidades relevantes
        entity_certainty = min(len(intent.entities) * 0.2, 1.0)
        certainty_factors.append(entity_certainty)
        
        # Calcular m√©dia ponderada
        weights = [0.3, 0.2, 0.2, 0.15, 0.15]  # Soma = 1.0
        weighted_certainty = sum(factor * weight for factor, weight in zip(certainty_factors, weights))
        
        return min(weighted_certainty, 1.0)
    
    async def _update_ultra_conversation_memory(self, phone: str, intent: ContextualIntent, text: str, linguistic_analysis: Dict):
        """üß† ATUALIZAR MEM√ìRIA ULTRA AVAN√áADA"""
        memory = self.conversation_memories[phone]
        
        # Atualizar hist√≥rico de inten√ß√µes
        memory.intent_history.append((
            intent.intent.value, 
            intent.confidence, 
            datetime.now()
        ))
        
        # Atualizar jornada emocional
        memory.emotional_journey.append((
            intent.emotional_state,
            intent.confidence,
            datetime.now()
        ))
        
        # Atualizar padr√µes de conversa√ß√£o
        memory.conversation_patterns.append(text[:100])  # Primeiros 100 chars
        
        # Detectar mudan√ßas de contexto
        if len(memory.intent_history) > 1:
            last_intent = memory.intent_history[-2][0]
            if intent.intent.value != last_intent:
                if intent.contextual_coherence < 0.4:  # Mudan√ßa abrupta
                    memory.context_switches.append(datetime.now())
        
        # Atualizar dados de aprendizado
        memory.learning_data['total_messages'] = memory.learning_data.get('total_messages', 0) + 1
        memory.learning_data['avg_confidence'] = (
            memory.learning_data.get('avg_confidence', 0.5) + intent.confidence
        ) / 2
        
        # Manter apenas √∫ltimos 50 registros de cada tipo
        memory.intent_history = memory.intent_history[-50:]
        memory.emotional_journey = memory.emotional_journey[-50:]
        memory.conversation_patterns = memory.conversation_patterns[-50:]
        memory.context_switches = memory.context_switches[-20:]
    
    async def _learn_from_interaction(self, phone: str, intent: ContextualIntent, semantic_analysis: Dict):
        """üéì APRENDER A PARTIR DA INTERA√á√ÉO"""
        # Armazenar padr√µes bem-sucedidos para aprendizado futuro
        if intent.confidence > 0.8:
            pattern_key = f"{intent.intent.value}_{intent.emotional_state}"
            
            if pattern_key not in self.pattern_learning_db:
                self.pattern_learning_db[pattern_key] = []
            
            # Armazenar caracter√≠sticas da mensagem bem entendida
            learning_pattern = {
                'semantic_clusters': intent.semantic_clusters,
                'entities_count': len(intent.entities),
                'discourse_markers': intent.discourse_markers,
                'confidence': intent.confidence,
                'timestamp': datetime.now()
            }
            
            self.pattern_learning_db[pattern_key].append(learning_pattern)
            
            # Manter apenas √∫ltimos 20 padr√µes por tipo
            self.pattern_learning_db[pattern_key] = self.pattern_learning_db[pattern_key][-20:]
    
    async def _generate_ultra_contextual_response(
        self, phone: str, intent: ContextualIntent, entities: List[ExtractedEntity], 
        memory: ConversationMemory, semantic_analysis: Dict
    ) -> Dict[str, Any]:
        """üé≠ GERA√á√ÉO ULTRA INTELIGENTE DE RESPOSTA N√çVEL CHATGPT"""
        
        # Usar gerador existente como base
        base_response = await self._generate_contextual_response(phone, intent, entities, {})
        
        # ENRIQUECER COM INTELIG√äNCIA ULTRA AVAN√áADA
        
        # 1. Personaliza√ß√£o baseada em mem√≥ria
        personalization = self._generate_personalized_elements(memory, intent)
        
        # 2. Adapta√ß√£o baseada em certeza
        certainty_adaptation = self._adapt_response_for_certainty(intent.intent_certainty)
        
        # 3. Contextualiza√ß√£o sem√¢ntica
        semantic_context = self._add_semantic_context(semantic_analysis, intent)
        
        # 4. Resposta din√¢mica baseada em padr√µes aprendidos
        learned_enhancements = self._apply_learned_patterns(intent, memory)
        
        # Gerar resposta ultra contextualizada
        ultra_response_text = self._compose_ultra_response(
            base_response['text'], personalization, certainty_adaptation, 
            semantic_context, learned_enhancements, intent
        )
        
        return {
            'text': ultra_response_text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'entities_detected': len(entities),
            'emotional_state': intent.emotional_state,
            'multiple_intents': len(intent.multiple_intents),
            'context_enhanced': True,
            'response_type': 'ultra_mega_contextual',
            
            # NOVOS CAMPOS ULTRA AVAN√áADOS
            'semantic_similarity': intent.semantic_similarity,
            'contextual_coherence': intent.contextual_coherence,
            'intent_certainty': intent.intent_certainty,
            'personalization_level': len(personalization),
            'semantic_clusters': intent.semantic_clusters,
            'discourse_markers': intent.discourse_markers,
            'ultra_enhanced': True
        }
    
    def _generate_personalized_elements(self, memory: ConversationMemory, intent: ContextualIntent) -> Dict[str, str]:
        """üë§ GERAR ELEMENTOS PERSONALIZADOS"""
        personalization = {}
        
        # Baseado em padr√£o emocional
        if memory.emotional_journey:
            recent_emotions = [emotion for emotion, _, _ in memory.emotional_journey[-3:]]
            if recent_emotions.count('frustrado') >= 2:
                personalization['empathy'] = "Eu vejo que voc√™ est√° passando por uma situa√ß√£o chata"
            elif recent_emotions.count('urgente') >= 2:
                personalization['urgency_ack'] = "Entendo que isso √© urgente para voc√™"
        
        # Baseado em hist√≥rico de inten√ß√µes
        if memory.intent_history:
            common_intents = Counter([intent for intent, _, _ in memory.intent_history])
            most_common = common_intents.most_common(1)[0][0]
            if most_common == 'fatura_solicitar':
                personalization['context'] = "Como sempre, vou buscar sua fatura"
        
        return personalization
    
    def _adapt_response_for_certainty(self, certainty: float) -> Dict[str, str]:
        """‚úÖ ADAPTAR RESPOSTA BASEADA NA CERTEZA"""
        if certainty > 0.9:
            return {'confidence_marker': '**CERTEZA ABSOLUTA!**', 'action': 'Vou resolver isso AGORA!'}
        elif certainty > 0.7:
            return {'confidence_marker': '**ENTENDI PERFEITAMENTE!**', 'action': 'Vou cuidar disso!'}
        elif certainty > 0.5:
            return {'confidence_marker': '**ACHO QUE ENTENDI!**', 'action': 'Deixe-me confirmar...'}
        else:
            return {'confidence_marker': '**HMMMM...**', 'action': 'Me explica melhor?'}
    
    def _add_semantic_context(self, semantic_analysis: Dict, intent: ContextualIntent) -> Dict[str, str]:
        """üß† ADICIONAR CONTEXTO SEM√ÇNTICO"""
        context = {}
        
        if semantic_analysis.get('best_match'):
            pattern_id, score = semantic_analysis['best_match']
            if score > 0.8:
                context['semantic_confidence'] = f"Detectei {int(score*100)}% de certeza"
        
        return context
    
    def _apply_learned_patterns(self, intent: ContextualIntent, memory: ConversationMemory) -> Dict[str, str]:
        """üéì APLICAR PADR√ïES APRENDIDOS"""
        enhancements = {}
        
        pattern_key = f"{intent.intent.value}_{intent.emotional_state}"
        if pattern_key in self.pattern_learning_db:
            patterns = self.pattern_learning_db[pattern_key]
            if patterns:
                # Aplicar insights dos padr√µes aprendidos
                avg_confidence = sum(p['confidence'] for p in patterns) / len(patterns)
                if avg_confidence > 0.8:
                    enhancements['learned_boost'] = "Baseado no que aprendi com voc√™"
        
        return enhancements
    
    def _compose_ultra_response(
        self, base_text: str, personalization: Dict, certainty: Dict, 
        semantic: Dict, learned: Dict, intent: ContextualIntent
    ) -> str:
        """üé≠ COMPOR RESPOSTA ULTRA AVAN√áADA"""
        
        # Come√ßar com texto base
        response_parts = [base_text]
        
        # Adicionar personaliza√ß√£o
        if personalization.get('empathy'):
            response_parts.insert(0, personalization['empathy'] + ".")
        
        # Adicionar marcador de confian√ßa
        if certainty.get('confidence_marker'):
            response_parts[0] = response_parts[0].replace(
                response_parts[0].split()[0], 
                certainty['confidence_marker']
            )
        
        # Adicionar contexto sem√¢ntico se alta confian√ßa
        if semantic.get('semantic_confidence'):
            response_parts.append(f"\n\nüéØ {semantic['semantic_confidence']} no que voc√™ quis dizer!")
        
        # Adicionar insights aprendidos
        if learned.get('learned_boost'):
            response_parts.append(f"\n\nüß† {learned['learned_boost']}, sei exatamente o que fazer!")
        
        return " ".join(response_parts)
    
    async def _ultra_intelligent_fallback(self, phone: str, text: str, error: Exception) -> Dict[str, Any]:
        """üõ°Ô∏è FALLBACK ULTRA INTELIGENTE MULTI-CAMADAS"""
        
        logger.error(f"üöÄ Ativando fallback ultra inteligente para: {text[:50]}... | Erro: {error}")
        
        # Tentar fallbacks em cascata
        for fallback_level in self.intelligent_fallback_system['fallback_levels']:
            try:
                if fallback_level['name'] == 'semantic_similarity':
                    return await self._fallback_semantic_similarity(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'keyword_extraction':
                    return await self._fallback_keyword_extraction(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'pattern_matching':
                    return await self._fallback_pattern_matching(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'conversational_context':
                    return await self._fallback_conversational_context(phone, text, fallback_level['threshold'])
                elif fallback_level['name'] == 'intelligent_guess':
                    return await self._fallback_intelligent_guess(text, fallback_level['threshold'])
                    
            except Exception as fallback_error:
                logger.warning(f"Fallback n√≠vel {fallback_level['level']} falhou: {fallback_error}")
                continue
        
        # Fallback final de emerg√™ncia
        return {
            'text': "ü§î **NOSSA!** Essa foi dif√≠cil at√© para mim! Pode tentar falar de um jeito mais simples? Tipo: 'quero minha conta' ou 'quanto devo'?",
            'intent': 'emergency_fallback',
            'confidence': 0.1,
            'fallback_level': 'emergency',
            'ultra_enhanced': True
        }
    
    async def _fallback_semantic_similarity(self, text: str, threshold: float) -> Dict[str, Any]:
        """üîç FALLBACK POR SIMILARIDADE SEM√ÇNTICA"""
        # Tentar encontrar padr√£o sem√¢ntico mais pr√≥ximo
        best_match = None
        best_score = 0.0
        
        for pattern_id, pattern in self.semantic_patterns.items():
            score = self._calculate_semantic_similarity(text, pattern)
            if score > best_score and score > threshold:
                best_match = pattern_id
                best_score = score
        
        if best_match:
            intent_mapping = {
                'fatura_semantic': 'fatura_solicitar',
                'valor_semantic': 'fatura_valor',
                'tempo_semantic': 'fatura_vencimento',
                'negociacao_semantic': 'negociacao_parcelamento'
            }
            
            inferred_intent = intent_mapping.get(best_match, 'fatura_solicitar')
            
            return {
                'text': f"üéØ **ENTENDI PELO CONTEXTO!** Voc√™ quer algo relacionado a {inferred_intent.replace('_', ' ')}. Vou ajudar!",
                'intent': inferred_intent,
                'confidence': best_score,
                'fallback_level': 'semantic_similarity',
                'ultra_enhanced': True
            }
        
        raise Exception("Similaridade sem√¢ntica insuficiente")
    
    async def _fallback_keyword_extraction(self, text: str, threshold: float) -> Dict[str, Any]:
        """üîë FALLBACK POR EXTRA√á√ÉO DE PALAVRAS-CHAVE"""
        keywords = {
            'fatura': ['conta', 'boleto', 'fatura', 'segunda', 'via', 'papel'],
            'valor': ['quanto', 'valor', 'devo', 'pagar', 'pre√ßo', 'dinheiro'],
            'vencimento': ['quando', 'vence', 'prazo', 'data', 'at√©'],
            'negociacao': ['parcelar', 'acordo', 'desconto', 'negociar', 'facilitar']
        }
        
        scores = {}
        for intent, intent_keywords in keywords.items():
            score = sum(1 for keyword in intent_keywords if keyword in text.lower())
            if score > 0:
                scores[intent] = score / len(intent_keywords)
        
        if scores:
            best_intent = max(scores.items(), key=lambda x: x[1])
            if best_intent[1] > threshold:
                return {
                    'text': f"üîç **CAPTEI!** Pelas palavras-chave, voc√™ quer {best_intent[0]}. √â isso mesmo?",
                    'intent': best_intent[0],
                    'confidence': best_intent[1],
                    'fallback_level': 'keyword_extraction',
                    'ultra_enhanced': True
                }
        
        raise Exception("Palavras-chave insuficientes")
    
    async def _fallback_pattern_matching(self, text: str, threshold: float) -> Dict[str, Any]:
        """üß© FALLBACK POR CORRESPOND√äNCIA DE PADR√ïES"""
        # Padr√µes de emerg√™ncia muito b√°sicos
        emergency_patterns = [
            (r'\b(conta|boleto|fatura)\b', 'fatura_solicitar', 0.7),
            (r'\b(quanto|valor)\b', 'fatura_valor', 0.6),
            (r'\b(quando|vence|prazo)\b', 'fatura_vencimento', 0.6),
            (r'\b(paguei|pago)\b', 'pagamento_confirmacao', 0.5),
            (r'\b(parcelar|acordo)\b', 'negociacao_parcelamento', 0.5),
        ]
        
        for pattern, intent, confidence in emergency_patterns:
            if re.search(pattern, text, re.IGNORECASE) and confidence > threshold:
                return {
                    'text': f"üß© **CONSEGUI ENTENDER!** Pelo padr√£o, voc√™ quer {intent.replace('_', ' ')}!",
                    'intent': intent,
                    'confidence': confidence,
                    'fallback_level': 'pattern_matching',
                    'ultra_enhanced': True
                }
        
        raise Exception("Nenhum padr√£o corresponde")
    
    async def _fallback_conversational_context(self, phone: str, text: str, threshold: float) -> Dict[str, Any]:
        """üí≠ FALLBACK POR CONTEXTO CONVERSACIONAL"""
        if phone in self.conversation_memories:
            memory = self.conversation_memories[phone]
            if memory.intent_history:
                # Assumir que √© follow-up da √∫ltima inten√ß√£o
                last_intent, last_confidence, _ = memory.intent_history[-1]
                
                if last_confidence > threshold:
                    return {
                        'text': f"üí≠ **PELO CONTEXTO!** Voc√™ ainda est√° falando sobre {last_intent.replace('_', ' ')}, n√©?",
                        'intent': last_intent,
                        'confidence': last_confidence * 0.8,
                        'fallback_level': 'conversational_context',
                        'ultra_enhanced': True
                    }
        
        raise Exception("Contexto conversacional insuficiente")
    
    async def _fallback_intelligent_guess(self, text: str, threshold: float) -> Dict[str, Any]:
        """üé≤ FALLBACK POR SUPOSI√á√ÉO INTELIGENTE"""
        # Se chegou at√© aqui, fazer uma suposi√ß√£o educada baseada no contexto de cobran√ßa
        text_length = len(text.split())
        
        if text_length <= 3:
            # Texto muito curto - provavelmente quer fatura
            guess_intent = 'fatura_solicitar'
            guess_confidence = 0.4
        elif '?' in text:
            # Tem pergunta - provavelmente quer informa√ß√£o (valor ou vencimento)
            guess_intent = 'fatura_valor'
            guess_confidence = 0.3
        else:
            # Default para solicita√ß√£o de fatura
            guess_intent = 'fatura_solicitar'
            guess_confidence = 0.2
        
        if guess_confidence > threshold:
            return {
                'text': f"üé≤ **VAMOS TENTAR!** Pelo contexto geral, acho que voc√™ quer {guess_intent.replace('_', ' ')}. Se n√£o for isso, me fala 'n√£o' que eu entendo outra coisa!",
                'intent': guess_intent,
                'confidence': guess_confidence,
                'fallback_level': 'intelligent_guess',
                'ultra_enhanced': True,
                'requires_confirmation': True
            }
        
        raise Exception("Imposs√≠vel fazer suposi√ß√£o v√°lida") 
            negociacao_parcelamento_score += 0.2
        
        # Se tem datas, boost vencimento
        tem_data = any(e.type == 'datas' for e in entities)
        if tem_data:
            fatura_vencimento_score += 0.4
            pagamento_score += 0.2
        
        # üò§ BOOST BASEADO EM EMO√á√ÉO
        if emotion == 'frustrado':
            reclamacao_indevida_score += 0.4
            reclamacao_valor_score += 0.4
        elif emotion == 'urgente':
            fatura_solicitar_score += 0.3
            fatura_valor_score += 0.2
        elif emotion == 'confuso':
            duvida_score += 0.3
        
        # üéØ NORMALIZAR SCORES (max 1.0 para cada)
        intent_scores = {
            'fatura_solicitar': min(fatura_solicitar_score, 1.0),
            'fatura_valor': min(fatura_valor_score, 1.0),
            'fatura_vencimento': min(fatura_vencimento_score, 1.0),
            'negociacao_parcelamento': min(negociacao_parcelamento_score, 1.0),
            'negociacao_desconto': min(negociacao_desconto_score, 1.0),
            'pagamento_confirmacao': min(pagamento_score, 1.0),
            'reclamacao_cobranca_indevida': min(reclamacao_indevida_score, 1.0),
            'reclamacao_valor_incorreto': min(reclamacao_valor_score, 1.0),
            'saudacao': min(saudacao_score, 1.0),
            'despedida': min(despedida_score, 1.0),
            'confirmacao': min(confirmacao_score, 1.0),
            'negacao': min(negacao_score, 1.0),
            'duvida': min(duvida_score, 1.0)
        }
        
        # üö® FALLBACK INTELIGENTE - Se nenhuma inten√ß√£o forte foi detectada
        max_score = max(intent_scores.values()) if intent_scores.values() else 0
        if max_score < 0.3:
            # Cliente escreveu algo muito confuso - tentar inferir pela presen√ßa de palavras-chave
            if any(palavra in text.lower() for palavra in ['conta', 'boleto', 'fatura', 'pagar', 'deve']):
                intent_scores['fatura_solicitar'] = 0.5  # Assumir que quer fatura
            elif any(palavra in text.lower() for palavra in ['quanto', 'valor', 'pre√ßo']):
                intent_scores['fatura_valor'] = 0.5  # Assumir que quer saber valor
            else:
                intent_scores['duvida'] = 0.5  # Cliente est√° confuso
        
        return intent_scores
    
    def _apply_contextual_boost(self, base_intents: Dict[str, float], context: Dict) -> Dict[str, float]:
        """Aplicar boost baseado no contexto conversacional"""
        boosted_intents = base_intents.copy()
        
        # Se √∫ltima mensagem foi sobre fatura, boost relacionados
        last_context = context.get('last_intent')
        if last_context and 'fatura' in last_context:
            boosted_intents['fatura_valor'] = boosted_intents.get('fatura_valor', 0) + 0.2
            boosted_intents['fatura_vencimento'] = boosted_intents.get('fatura_vencimento', 0) + 0.2
        
        # Se contexto de negocia√ß√£o ativa
        if context.get('negotiation_active'):
            boosted_intents['negociacao_desconto'] = boosted_intents.get('negociacao_desconto', 0) + 0.3
            boosted_intents['confirmacao'] = boosted_intents.get('confirmacao', 0) + 0.2
        
        return boosted_intents
    
    def _detect_multiple_intents(self, text: str, entities: List[ExtractedEntity]) -> List[IntentType]:
        """Detectar m√∫ltiplas inten√ß√µes na mesma mensagem - MELHORADO"""
        intents = []
        
        # Detectores mais robustos de m√∫ltiplas inten√ß√µes
        
        # "fatura E desconto/parcelamento"
        if (re.search(r'(fatura|conta)', text, re.IGNORECASE) and 
            re.search(r'(tamb√©m|e\s+(tamb√©m)?).*(desconto|parcelar)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_DESCONTO])
        
        # "fatura E parcelamento"  
        if (re.search(r'(fatura|conta)', text, re.IGNORECASE) and 
            re.search(r'(tamb√©m|e\s+(tamb√©m)?).*(parcelar|dividir)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_PARCELAMENTO])
        
        # "paguei MAS ainda aparece"
        if (re.search(r'(paguei|quitei)', text, re.IGNORECASE) and 
            re.search(r'(mas|por√©m|ainda|continua|aparece)', text, re.IGNORECASE)):
            intents.extend([IntentType.PAGAMENTO_CONFIRMACAO, IntentType.RECLAMACAO_VALOR_INCORRETO])
        
        # "valor E vencimento"
        if (re.search(r'(quanto.*devo)', text, re.IGNORECASE) and 
            re.search(r'(quando.*vence|prazo)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_VALOR, IntentType.FATURA_VENCIMENTO])
        
        # Conectores brasileiros comuns
        conectores = [r'\s+e\s+', r'\s+tamb√©m\s+', r'\s+al√©m\s+disso\s+', r'\s+mais\s+']
        for conector in conectores:
            if re.search(conector, text, re.IGNORECASE):
                # Se tem conector, analisar cada parte
                partes = re.split(conector, text, flags=re.IGNORECASE)
                if len(partes) >= 2:
                    # Analisar se cada parte tem inten√ß√£o diferente
                    primeira_parte = partes[0].strip()
                    segunda_parte = partes[1].strip()
                    
                    # L√≥gica simplificada para detectar inten√ß√µes diferentes
                    if ('fatura' in primeira_parte.lower() and 
                        any(palavra in segunda_parte.lower() for palavra in ['desconto', 'parcelar', 'negociar'])):
                        intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_DESCONTO])
                        break
        
        return intents
    
    # M√©todos de normaliza√ß√£o (implementa√ß√µes simplificadas)
    def _normalize_currency(self, text: str) -> str:
        return re.sub(r'[^\d,]', '', text)
    
    def _normalize_date(self, text: str) -> str:
        return text.strip()
    
    def _normalize_protocol(self, text: str) -> str:
        return re.sub(r'[^\w\d]', '', text)
    
    def _normalize_document(self, text: str) -> str:
        return re.sub(r'[^\d]', '', text)
    
    def _super_normalize_text(self, text: str) -> str:
        """üöÄ NORMALIZA√á√ÉO ULTRA AVAN√áADA - CORRIGE QUALQUER TEXTO MAL ESCRITO"""
        
        # 1. PRIMEIRA PASSADA - Limpeza b√°sica
        text = text.lower().strip()
        
        # 2. REMOVER EMOJIS E CARACTERES ESPECIAIS (mas preservar pontua√ß√£o b√°sica)
        text = re.sub(r'[^\w\s\.,!?\-√°√†√¢√£√©√®√™√≠√¨√Æ√≥√≤√¥√µ√∫√π√ª√ß]', ' ', text)
        
        # 3. CORRIGIR ABREVIA√á√ïES E ERROS COMUNS (do nosso dicion√°rio)
        erro_patterns = self.brazilian_language_db.get('erro_patterns', {})
        for erro, correto in erro_patterns.items():
            # Usar word boundary para n√£o corrigir partes de palavras
            text = re.sub(rf'\b{re.escape(erro)}\b', correto, text, flags=re.IGNORECASE)
        
        # 4. CORRE√á√ïES ESPEC√çFICAS DE PORTUGU√äS BRASILEIRO MAL ESCRITO
        corrections = {
            # Erros comuns de "quanto"
            r'\b(qnt|qnto|qto|cuanto)\b': 'quanto',
            # Erros comuns de "quando"  
            r'\b(qnd|qndo|quado|cuando)\b': 'quando',
            # Erros de "voc√™"
            r'\bvc\b': 'voc√™',
            # Erros de "n√£o"
            r'\b(nao|naum|√±|n)\b': 'n√£o',
            # Erros de "para"
            r'\b(pra|pr)\b': 'para',
            # Erros de "porque"
            r'\b(pq|pk|porq)\b': 'porque',
            # Erros de "tamb√©m"
            r'\b(tb|tbm|tbn)\b': 'tamb√©m',
            # Erros de "est√°"
            r'\b(tah|ta|t√°)\b': 'est√°',
            # Erros de "estou"
            r'\b(to|tou)\b': 'estou',
            # Erros de "j√°"
            r'\b(jah|ja)\b': 'j√°',
            # Erros de "s√≥"
            r'\b(soh|so)\b': 's√≥',
            # Erros de "√©"
            r'\b(eh|e)\b': '√©',
            # Erros de "hoje"
            r'\bhj\b': 'hoje',
            # Erros de "amanh√£"
            r'\b(amanha|am√±)\b': 'amanh√£',
            # Erros de "cad√™"
            r'\bkd\b': 'cad√™',
            # Erros de "aqui"
            r'\b(aki|aq)\b': 'aqui',
            # Erros de "a√≠"
            r'\b(ai|ae)\b': 'a√≠',
            # Erros de "mesmo"
            r'\b(msm|mmo)\b': 'mesmo',
            # Erros de "beleza"
            r'\b(blz|bz)\b': 'beleza',
            # Erros de "valeu"
            r'\b(vlw|vl)\b': 'valeu',
            # Erros de "falou"
            r'\b(flw|fl)\b': 'falou',
            # Erros comuns de palavras de cobran√ßa
            r'\b(fatur|ftur)\b': 'fatura',
            r'\b(bolto|bleto)\b': 'boleto',
            r'\b(cota|cnta)\b': 'conta',
            r'\b(cobransa|cobranca)\b': 'cobran√ßa',
            r'\b(pagameto|pagamnto)\b': 'pagamento',
            r'\b(vencimeto|vencimto)\b': 'vencimento',
            r'\b(transferencia|trasferencia)\b': 'transfer√™ncia',
            r'\b(debto|debito)\b': 'd√©bito'
        }
        
        for pattern, replacement in corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        # 5. REMOVER PONTUA√á√ÉO EXCESSIVA mas preservar sentido
        text = re.sub(r'[!]{2,}', '!', text)
        text = re.sub(r'[?]{2,}', '?', text)
        text = re.sub(r'[.]{2,}', '...', text)
        
        # 6. NORMALIZAR ESPA√áOS
        text = re.sub(r'\s+', ' ', text)
        
        # 7. CORRE√á√ïES CONTEXTUAIS ESPEC√çFICAS PARA COBRAN√áA
        cobranca_corrections = {
            # "segunda via" mal escrito
            r'(segunda|2)\s*(v|vi|via)': 'segunda via',
            # "quanto devo" mal escrito  
            r'(quanto|qnto)\s*(devo|dvo|dveo)': 'quanto devo',
            # "j√° paguei" mal escrito
            r'(j√°|jah|ja)\s*(paguei|pguei|pag)': 'j√° paguei',
            # "n√£o devo" mal escrito
            r'(n√£o|nao|naum)\s*(devo|dvo)': 'n√£o devo',
            # "minha conta" mal escrito
            r'(minha|miha)\s*(conta|cota)': 'minha conta'
        }
        
        for pattern, replacement in cobranca_corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def _get_conversation_context(self, phone: str) -> Dict:
        """Obter contexto da conversa"""
        return self.user_contexts.get(phone, {})
    
    def _update_conversation_context(self, phone: str, intent: ContextualIntent, text: str):
        """Atualizar contexto conversacional"""
        if phone not in self.user_contexts:
            self.user_contexts[phone] = {
                'messages': [],
                'last_intent': None,
                'negotiation_active': False,
                'client_profile': {},
                'conversation_flow': []
            }
        
        context = self.user_contexts[phone]
        context['messages'].append({
            'text': text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'timestamp': datetime.now(),
            'entities': [{'type': e.type, 'value': e.value} for e in intent.entities],
            'emotion': intent.emotional_state
        })
        
        context['last_intent'] = intent.intent.value
        
        # Detectar se negocia√ß√£o est√° ativa
        if intent.intent in [IntentType.NEGOCIACAO_DESCONTO, IntentType.NEGOCIACAO_PARCELAMENTO]:
            context['negotiation_active'] = True
        
        # Manter apenas √∫ltimas 10 mensagens
        context['messages'] = context['messages'][-10:]
    
    async def _generate_contextual_response(
        self, phone: str, intent: ContextualIntent, entities: List[ExtractedEntity], context: Dict
    ) -> Dict[str, Any]:
        """üöÄ GERADOR DE RESPOSTAS ULTRA INTELIGENTE - PERFEITO PARA CLIENTES BURROS"""
        
        # üéØ RESPOSTAS BASEADAS NA INTEN√á√ÉO COM CONTEXTO EMOCIONAL
        
        if intent.intent == IntentType.FATURA_SOLICITAR:
            if intent.emotional_state == 'urgente':
                response_text = "üö® **URGENTE!** Entendi! Vou buscar sua fatura AGORA MESMO e te enviar em segundos!"
            elif intent.emotional_state == 'frustrado':
                response_text = "üòî Percebo que voc√™ est√° chateado. Calma, vou resolver isso rapidinho! Enviando sua fatura j√°..."
            elif intent.negation:
                response_text = "ü§î Vi que voc√™ disse 'n√£o' sobre algo. Me explica melhor o que voc√™ precisa da sua conta?"
            else:
                response_text = "üìÑ **PERFEITO!** Vou pegar sua fatura para voc√™. S√≥ um minutinho..."
        
        elif intent.intent == IntentType.FATURA_VALOR:
            valor_entity = next((e for e in entities if e.type == 'valores_monetarios'), None)
            if valor_entity:
                response_text = f"üí∞ Vi que voc√™ mencionou **R$ {valor_entity.value}**. Vou confirmar se esse √© o valor correto da sua conta!"
            elif intent.emotional_state == 'urgente':
                response_text = "üí∞ **URGENTE!** Vou verificar AGORA quanto voc√™ deve exatamente!"
            else:
                response_text = "üí∞ Entendi! Voc√™ quer saber **QUANTO DEVE**, certo? Vou verificar o valor da sua conta!"
        
        elif intent.intent == IntentType.FATURA_VENCIMENTO:
            data_entity = next((e for e in entities if e.type == 'datas'), None)
            if data_entity:
                response_text = f"‚è∞ Vi que voc√™ mencionou **{data_entity.value}**. Vou confirmar o vencimento da sua conta!"
            else:
                response_text = "‚è∞ Entendi! Voc√™ quer saber **QUANDO VENCE** sua conta, n√©? Vou verificar a data!"
        
        elif intent.intent == IntentType.NEGOCIACAO_PARCELAMENTO:
            if intent.emotional_state == 'frustrado':
                response_text = "ü§ù Entendo que est√° dif√≠cil pagar. **CALMA!** Vamos dar um jeito! Temos v√°rias op√ß√µes de parcelamento!"
            elif any(e.type == 'valores_monetarios' for e in entities):
                valor = next(e.value for e in entities if e.type == 'valores_monetarios')
                response_text = f"ü§ù Perfeito! Voc√™ quer parcelar **R$ {valor}**, n√©? Vamos encontrar a melhor condi√ß√£o para voc√™!"
            else:
                response_text = "ü§ù **√ìTIMO!** Quer parcelar? Vou ver as melhores condi√ß√µes que temos dispon√≠veis!"
        
        elif intent.intent == IntentType.NEGOCIACAO_DESCONTO:
            if intent.emotional_state == 'frustrado':
                response_text = "üí∏ Entendo sua situa√ß√£o! Vamos ver que **DESCONTO** posso conseguir para voc√™!"
            else:
                response_text = "üí∏ Interessado em desconto? **PERFEITO!** Vou verificar as promo√ß√µes dispon√≠veis!"
        
        elif intent.intent == IntentType.PAGAMENTO_CONFIRMACAO:
            if intent.temporal_context == 'passado':
                if intent.emotional_state == 'frustrado':
                    response_text = "‚úÖ Entendi! Voc√™ **J√Å PAGOU** mas ainda est√° aparecendo, n√©? Vou verificar URGENTE o que aconteceu!"
            else:
                    response_text = "‚úÖ **BELEZA!** Voc√™ j√° pagou! Vou confirmar aqui no sistema se o pagamento foi processado!"
            else:
                response_text = "üí≥ Perfeito! Vou verificar o status do seu pagamento no sistema!"
        
        elif intent.intent == IntentType.RECLAMACAO_COBRANCA_INDEVIDA:
            if intent.emotional_state == 'frustrado':
                response_text = "üò° **ENTENDO SUA REVOLTA!** Cobran√ßa indevida √© muito chato mesmo! Vou resolver isso AGORA!"
            else:
                response_text = "üîç Entendi! Voc√™ acha que essa cobran√ßa est√° **ERRADA**, n√©? Vou analisar sua situa√ß√£o!"
        
        elif intent.intent == IntentType.RECLAMACAO_VALOR_INCORRETO:
            response_text = "üîç **NOSSA!** Valor incorreto √© s√©rio! Vou verificar sua conta e corrigir se estiver errado mesmo!"
        
        elif intent.intent == IntentType.SAUDACAO:
            horario = datetime.now().hour
            if horario < 12:
                response_text = "üåÖ **BOM DIA!** Tudo beleza? Como posso te ajudar hoje?"
            elif horario < 18:
                response_text = "‚òÄÔ∏è **BOA TARDE!** E a√≠, tudo certo? Em que posso ajudar?"
        else:
                response_text = "üåô **BOA NOITE!** Beleza? Como posso te ajudar?"
        
        elif intent.intent == IntentType.DESPEDIDA:
            response_text = "üëã **VALEU!** Obrigado pelo contato! Qualquer coisa, me chama! üòä"
        
        elif intent.intent == IntentType.CONFIRMACAO:
            response_text = "‚úÖ **PERFEITO!** Entendi que voc√™ confirmou! Vou continuar com o processo!"
        
        elif intent.intent == IntentType.NEGACAO:
            response_text = "‚ùå **BELEZA!** Voc√™ disse que n√£o. Me explica melhor o que voc√™ precisa ent√£o?"
        
        elif intent.intent == IntentType.DUVIDA:
            response_text = "ü§î **SEM PROBLEMAS!** Vou explicar melhor! O que especificamente voc√™ n√£o entendeu?"
        
        else:
            # Fallback inteligente baseado no que foi detectado
            if intent.confidence < 0.5:
                response_text = "ü§î **CALMA!** Acho que n√£o entendi direito. Pode me falar de novo de um jeito mais simples? Tipo: 'quero minha conta' ou 'quanto devo'?"
            else:
                response_text = "ü§ñ **ENTENDI ALGUMA COISA!** Mas me explica melhor o que voc√™ precisa. Fala de forma simples!"
        
        # üìã ADICIONAR INFORMA√á√ïES SOBRE M√öLTIPLAS INTEN√á√ïES
        if intent.multiple_intents and len(intent.multiple_intents) > 0:
            intents_text = []
            for multi_intent in intent.multiple_intents:
                if multi_intent == IntentType.FATURA_SOLICITAR:
                    intents_text.append("ver sua conta")
                elif multi_intent == IntentType.FATURA_VALOR:
                    intents_text.append("saber quanto deve")
                elif multi_intent == IntentType.NEGOCIACAO_PARCELAMENTO:
                    intents_text.append("parcelar")
                elif multi_intent == IntentType.NEGOCIACAO_DESCONTO:
                    intents_text.append("conseguir desconto")
                else:
                    intents_text.append(multi_intent.value.replace('_', ' '))
            
            if intents_text:
                response_text += f"\n\nüìã **TAMB√âM PERCEBI** que voc√™ quer: {' e '.join(intents_text)}. Vou ajudar com tudo!"
        
        # üî• ADICIONAR CALL TO ACTION BASEADO NA INTEN√á√ÉO
        if intent.intent in [IntentType.FATURA_SOLICITAR, IntentType.FATURA_VALOR, IntentType.FATURA_VENCIMENTO]:
            response_text += "\n\n‚ö° **Aguarda a√≠ que vou buscar suas informa√ß√µes!**"
        elif intent.intent in [IntentType.NEGOCIACAO_PARCELAMENTO, IntentType.NEGOCIACAO_DESCONTO]:
            response_text += "\n\nü§ù **Vou verificar as melhores condi√ß√µes para voc√™!**"
        elif intent.intent == IntentType.PAGAMENTO_CONFIRMACAO:
            response_text += "\n\nüîç **Verificando seu pagamento no sistema...**"
        
        return {
            'text': response_text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'entities_detected': len(entities),
            'emotional_state': intent.emotional_state,
            'multiple_intents': len(intent.multiple_intents),
            'context_enhanced': True,
            'response_type': 'ultra_contextual'
        }
    
    async def _generate_fallback_response(self, phone: str, text: str) -> Dict[str, Any]:
        """Resposta de fallback inteligente"""
        return {
            'text': "ü§î Percebi que voc√™ est√° tentando me dizer algo importante. Pode reformular para eu entender melhor?",
            'intent': 'clarification_needed',
            'confidence': 0.3,
            'fallback': True
        }
    
    def _load_contextual_responses(self) -> Dict:
        """Carregar respostas contextuais avan√ßadas"""
        return {
            # Implementar depois conforme necess√°rio
            'advanced_templates': {}
        } 
    
    # ================================
    # üöÄ SISTEMAS ULTRA AVAN√áADOS - N√çVEL CHATGPT
    # ================================
    
    def _build_semantic_patterns(self) -> Dict[str, SemanticPattern]:
        """üß† CONSTRUIR PADR√ïES SEM√ÇNTICOS ULTRA AVAN√áADOS"""
        patterns = {}
        
        # Padr√£o sem√¢ntico para FATURA
        patterns['fatura_semantic'] = SemanticPattern(
            pattern_id='fatura_semantic',
            semantic_vectors={
                'documento': 0.9, 'papel': 0.8, 'conta': 1.0, 'boleto': 1.0,
                'cobran√ßa': 0.9, 'd√©bito': 0.8, 'pagamento': 0.7, 'valor': 0.6,
                'segunda_via': 1.0, 'c√≥pia': 0.7, 'comprovante': 0.6
            },
            context_triggers=['preciso', 'quero', 'mandar', 'enviar', 'ver'],
            intent_weights={'fatura_solicitar': 1.0, 'fatura_valor': 0.3},
            emotional_indicators={'urgente': 0.3, 'neutro': 0.7},
            confidence_modifiers={'direto': 1.0, 'indireto': 0.7}
        )
        
        # Padr√£o sem√¢ntico para VALOR/QUANTIDADE
        patterns['valor_semantic'] = SemanticPattern(
            pattern_id='valor_semantic',
            semantic_vectors={
                'quanto': 1.0, 'valor': 1.0, 'pre√ßo': 0.9, 'custo': 0.8,
                'dinheiro': 0.7, 'grana': 0.8, 'real': 0.6, 'centavo': 0.5,
                'total': 0.9, 'dever': 0.9, 'pagar': 0.8
            },
            context_triggers=['devo', 'pago', 'custa', 'vale'],
            intent_weights={'fatura_valor': 1.0, 'pagamento_confirmacao': 0.4},
            emotional_indicators={'frustrado': 0.2, 'neutro': 0.8},
            confidence_modifiers={'pergunta': 1.0, 'afirmacao': 0.6}
        )
        
        # Padr√£o sem√¢ntico para TEMPO/VENCIMENTO
        patterns['tempo_semantic'] = SemanticPattern(
            pattern_id='tempo_semantic',
            semantic_vectors={
                'quando': 1.0, 'data': 0.9, 'dia': 0.8, 'prazo': 1.0,
                'vencimento': 1.0, 'vence': 1.0, 'at√©': 0.7, 'tempo': 0.8,
                'hoje': 0.6, 'amanh√£': 0.7, 'm√™s': 0.6
            },
            context_triggers=['vence', 'termina', 'acaba', 'expira'],
            intent_weights={'fatura_vencimento': 1.0, 'pagamento_confirmacao': 0.3},
            emotional_indicators={'urgente': 0.5, 'neutro': 0.5},
            confidence_modifiers={'futuro': 1.0, 'passado': 0.4}
        )
        
        # Padr√£o sem√¢ntico para NEGOCIA√á√ÉO
        patterns['negociacao_semantic'] = SemanticPattern(
            pattern_id='negociacao_semantic',
            semantic_vectors={
                'parcelar': 1.0, 'dividir': 0.9, 'acordo': 0.9, 'negociar': 1.0,
                'desconto': 1.0, 'abatimento': 0.8, 'facilitar': 0.7, 'ajuda': 0.6,
                'dificuldade': 0.8, 'problema': 0.7, 'apertado': 0.8, 'quebrar_galho': 0.9
            },
            context_triggers=['n√£o_consigo', 'dif√≠cil', 'sem_dinheiro', 'ajudar'],
            intent_weights={'negociacao_parcelamento': 0.7, 'negociacao_desconto': 0.3},
            emotional_indicators={'frustrado': 0.6, 'urgente': 0.4},
            confidence_modifiers={'pedido': 1.0, 'sugestao': 0.8}
        )
        
        return patterns
    
    def _build_semantic_vectors(self) -> Dict[str, Dict[str, float]]:
        """üî¨ CONSTRUIR VETORES SEM√ÇNTICOS BRASILEIROS ULTRA AVAN√áADOS"""
        return {
            # Vetores sem√¢nticos para palavras de cobran√ßa
            'fatura': {
                'conta': 0.95, 'boleto': 0.90, 'cobran√ßa': 0.85, 'd√©bito': 0.80,
                'documento': 0.75, 'papel': 0.70, 'segunda_via': 0.95, 'c√≥pia': 0.60
            },
            'pagar': {
                'quitar': 0.90, 'saldar': 0.85, 'liquidar': 0.80, 'acertar': 0.75,
                'resolver': 0.70, 'transferir': 0.65, 'depositar': 0.60
            },
            'quanto': {
                'valor': 0.95, 'pre√ßo': 0.90, 'custo': 0.85, 'total': 0.80,
                'dinheiro': 0.75, 'grana': 0.80, 'real': 0.70
            },
            'quando': {
                'data': 0.90, 'dia': 0.85, 'prazo': 0.95, 'vencimento': 0.95,
                'tempo': 0.80, 'at√©': 0.75, 'hora': 0.70
            },
            'problema': {
                'dificuldade': 0.90, 'complica√ß√£o': 0.85, 'erro': 0.80,
                'confus√£o': 0.75, 'encrenca': 0.85, 'pepino': 0.80
            }
        }
    
    def _build_intent_similarity_matrix(self) -> Dict[str, Dict[str, float]]:
        """üéØ MATRIZ DE SIMILARIDADE ENTRE INTEN√á√ïES"""
        return {
            'fatura_solicitar': {
                'fatura_valor': 0.7, 'fatura_vencimento': 0.6, 'pagamento_confirmacao': 0.4,
                'negociacao_parcelamento': 0.3, 'informacao_conta': 0.8
            },
            'fatura_valor': {
                'fatura_solicitar': 0.7, 'fatura_vencimento': 0.5, 'pagamento_confirmacao': 0.6,
                'negociacao_parcelamento': 0.7, 'negociacao_desconto': 0.5
            },
            'negociacao_parcelamento': {
                'negociacao_desconto': 0.8, 'pagamento_dificuldade': 0.9, 'fatura_valor': 0.6
            },
            'pagamento_confirmacao': {
                'reclamacao_valor_incorreto': 0.5, 'fatura_valor': 0.4, 'fatura_solicitar': 0.3
            }
        }
    
    def _build_relationship_graph(self) -> Dict[str, List[str]]:
        """üï∏Ô∏è GRAFO DE RELACIONAMENTOS CONTEXTUAIS"""
        return {
            'financial_entities': ['valor', 'dinheiro', 'real', 'centavo', 'pagar', 'dever'],
            'temporal_entities': ['quando', 'dia', 'data', 'prazo', 'vencimento', 'at√©'],
            'document_entities': ['conta', 'boleto', 'fatura', 'papel', 'documento', 'c√≥pia'],
            'negotiation_entities': ['parcelar', 'dividir', 'acordo', 'desconto', 'facilitar'],
            'emotional_entities': ['problema', 'dificuldade', 'urgente', 'chateado', 'nervoso'],
            'action_entities': ['quero', 'preciso', 'gostaria', 'mandar', 'enviar', 'ver']
        }
    
    def _load_discourse_analyzers(self) -> Dict[str, Any]:
        """üí¨ ANALISADORES DE DISCURSO ULTRA AVAN√áADOS"""
        return {
            'discourse_markers': {
                'addition': ['tamb√©m', 'al√©m disso', 'e', 'mais', 'ainda'],
                'contrast': ['mas', 'por√©m', 'entretanto', 'contudo', 'no entanto'],
                'cause': ['porque', 'pois', 'j√° que', 'visto que', 'uma vez que'],
                'conclusion': ['ent√£o', 'portanto', 'assim', 'logo', 'por isso'],
                'sequence': ['primeiro', 'depois', 'em seguida', 'finalmente', 'por √∫ltimo'],
                'emphasis': ['realmente', 'muito', 'bastante', 'extremamente', 'totalmente']
            },
            'pragmatic_markers': {
                'politeness': ['por favor', 'obrigado', 'desculpa', 'com licen√ßa'],
                'urgency': ['urgente', 'r√°pido', 'agora', 'imediatamente', 'j√°'],
                'uncertainty': ['acho', 'talvez', 'pode ser', 'n√£o tenho certeza'],
                'emphasis': ['realmente', 'certamente', 'definitivamente', 'com certeza']
            }
        }
    
    def _build_pragmatic_engine(self) -> Dict[str, Any]:
        """üß† ENGINE DE INFER√äNCIA PRAGM√ÅTICA ULTRA AVAN√áADA"""
        return {
            'implicature_rules': {
                # Se diz "j√° paguei MAS ainda aparece" = reclama valor incorreto
                'payment_but_still_charged': {
                    'pattern': r'(j√°.*pagu|quitei|paguei).*(mas|por√©m|ainda|continua)',
                    'inference': 'reclamacao_valor_incorreto',
                    'confidence': 0.9
                },
                # Se pergunta valor E prazo = quer informa√ß√µes completas
                'value_and_deadline': {
                    'pattern': r'(quanto.*devo).*(quando.*vence|prazo)',
                    'inference': 'multiple_intents',
                    'confidence': 0.8
                },
                # Se diz que n√£o consegue pagar = quer negociar
                'cannot_pay': {
                    'pattern': r'n√£o.*(consigo|posso).*(pagar|quitar)',
                    'inference': 'negociacao_parcelamento',
                    'confidence': 0.85
                }
            },
            'contextual_inference': {
                # Infer√™ncias baseadas no contexto da conversa
                'follow_up_questions': {
                    'after_invoice_request': ['fatura_valor', 'fatura_vencimento'],
                    'after_negotiation': ['confirmacao', 'negacao', 'duvida'],
                    'after_payment_info': ['pagamento_confirmacao']
                }
            }
        }
    
    def _build_coherence_analyzer(self) -> Dict[str, Any]:
        """üîó ANALISADOR DE COER√äNCIA CONTEXTUAL ULTRA AVAN√áADO"""
        return {
            'coherence_rules': {
                'topic_continuity': {
                    'same_topic': 1.0,      # Mesma inten√ß√£o que anterior
                    'related_topic': 0.8,   # Inten√ß√£o relacionada
                    'topic_shift': 0.4,     # Mudan√ßa de assunto
                    'random_topic': 0.1     # Assunto totalmente aleat√≥rio
                },
                'temporal_coherence': {
                    'logical_sequence': 1.0,    # Sequ√™ncia l√≥gica
                    'acceptable_jump': 0.7,     # Salto aceit√°vel
                    'confusing_sequence': 0.3   # Sequ√™ncia confusa
                }
            },
            'context_memory_window': 5,  # Quantas mensagens anteriores considerar
            'coherence_threshold': 0.6   # Limite m√≠nimo de coer√™ncia
        }
    
    def _build_multi_layer_processors(self) -> List[Dict[str, Any]]:
        """üéõÔ∏è PROCESSADORES MULTI-CAMADAS ULTRA AVAN√áADOS"""
        return [
            {
                'layer': 'lexical',
                'processor': 'word_level_analysis',
                'weight': 0.2,
                'functions': ['tokenization', 'pos_tagging', 'lemmatization']
            },
            {
                'layer': 'syntactic', 
                'processor': 'phrase_level_analysis',
                'weight': 0.3,
                'functions': ['phrase_detection', 'dependency_parsing']
            },
            {
                'layer': 'semantic',
                'processor': 'meaning_level_analysis', 
                'weight': 0.3,
                'functions': ['semantic_similarity', 'concept_mapping']
            },
            {
                'layer': 'pragmatic',
                'processor': 'context_level_analysis',
                'weight': 0.2,
                'functions': ['pragmatic_inference', 'discourse_analysis']
            }
        ]
    
    def _build_fallback_system(self) -> Dict[str, Any]:
        """üõ°Ô∏è SISTEMA DE FALLBACK INTELIGENTE MULTI-CAMADAS"""
        return {
            'fallback_levels': [
                {
                    'level': 1,
                    'name': 'semantic_similarity',
                    'method': 'find_closest_semantic_match',
                    'threshold': 0.6
                },
                {
                    'level': 2, 
                    'name': 'keyword_extraction',
                    'method': 'extract_key_concepts',
                    'threshold': 0.4
                },
                {
                    'level': 3,
                    'name': 'pattern_matching',
                    'method': 'fuzzy_pattern_match', 
                    'threshold': 0.3
                },
                {
                    'level': 4,
                    'name': 'conversational_context',
                    'method': 'infer_from_conversation',
                    'threshold': 0.2
                },
                {
                    'level': 5,
                    'name': 'intelligent_guess',
                    'method': 'make_educated_guess',
                    'threshold': 0.1
                }
            ]
        }
    
    def _build_dynamic_generator(self) -> Dict[str, Any]:
        """üé≠ GERADOR DIN√ÇMICO DE RESPOSTAS ULTRA INTELIGENTE"""
        return {
            'response_templates': {
                'high_confidence': "‚úÖ **{emotion_marker}** {action_confirmation} {specifics}",
                'medium_confidence': "ü§î **{understanding}** {clarification_request}",
                'low_confidence': "‚ùì **{confusion_acknowledgment}** {help_request}",
                'contextual': "üéØ **{context_reference}** {personalized_response}"
            },
            'emotion_markers': {
                'urgente': ['URGENTE!', 'RAPIDINHO!', 'AGORA MESMO!'],
                'frustrado': ['CALMA!', 'ENTENDO!', 'VAMOS RESOLVER!'],
                'neutro': ['PERFEITO!', 'BELEZA!', 'CERTO!'],
                'satisfeito': ['√ìTIMO!', 'EXCELENTE!', 'SHOW!']
            },
            'personalization_factors': [
                'conversation_history', 'emotional_state', 'communication_style',
                'previous_intents', 'response_patterns', 'user_preferences'
            ]
        }
    
    # ================================
    # üöÄ M√âTODOS ULTRA MEGA AVAN√áADOS - N√çVEL CHATGPT GIGANTEMENTE FODA
    # ================================
    
    def _get_or_create_conversation_memory(self, phone: str) -> ConversationMemory:
        """üß† OBTER OU CRIAR MEM√ìRIA ULTRA AVAN√áADA"""
        if phone not in self.conversation_memories:
            self.conversation_memories[phone] = ConversationMemory()
        return self.conversation_memories[phone]
    
    def _ultra_advanced_normalize_text(self, text: str) -> str:
        """üöÄ NORMALIZA√á√ÉO ULTRA MEGA AVAN√áADA"""
        # Usar o m√©todo existente mas com melhorias
        normalized = self._super_normalize_text(text)
        
        # Adicionar an√°lises extras ultra avan√ßadas
        normalized = self._apply_phonetic_corrections(normalized)
        normalized = self._fix_cognitive_errors(normalized)
        normalized = self._standardize_brazilian_expressions(normalized)
        
        return normalized
    
    def _apply_phonetic_corrections(self, text: str) -> str:
        """üîä CORRE√á√ïES FON√âTICAS ULTRA AVAN√áADAS"""
        phonetic_corrections = {
            # Corre√ß√µes baseadas em como as pessoas falam
            r'\b(di)\b': 'de',  # "di manh√£" -> "de manh√£"
            r'\b(nu)\b': 'no',  # "nu banco" -> "no banco"
            r'\b(du)\b': 'do',  # "du cliente" -> "do cliente"
            r'\b(ma)\b': 'mas', # "ma n√£o" -> "mas n√£o"
            r'\b(qui)\b': 'que', # "qui dia" -> "que dia"
            r'\b(cum√©)\b': 'como √©', # "cum√© que" -> "como √© que"
            r'\b(oc√™)\b': 'voc√™',    # "oc√™ tem" -> "voc√™ tem"
            r'\b(seje)\b': 'seja',   # "seje o que" -> "seja o que"
        }
        
        for pattern, replacement in phonetic_corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def _fix_cognitive_errors(self, text: str) -> str:
        """üß† CORRIGIR ERROS COGNITIVOS E DE RACIOC√çNIO"""
        cognitive_fixes = {
            # Erros de l√≥gica temporal
            r'(ontem.*amanha|amanha.*ontem)': 'ontem ou amanh√£',
            # Contradi√ß√µes √≥bvias
            r'(n√£o.*mas.*sim|sim.*mas.*n√£o)': 'talvez',
            # Confus√µes de pessoa
            r'(voc√™.*eu.*pagar|eu.*voc√™.*pagar)': 'preciso pagar',
        }
        
        for pattern, replacement in cognitive_fixes.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def _standardize_brazilian_expressions(self, text: str) -> str:
        """üáßüá∑ PADRONIZAR EXPRESS√ïES TIPICAMENTE BRASILEIRAS"""
        expressions = {
            r'(t√°.*ligado|sacou|entendeu)': 'entende',
            r'(massa|show|da.*hora)': 'bom',
            r'(trampo|labuta)': 'trabalho',
            r'(grana|din.*din|money)': 'dinheiro',
            r'(mina|mano|brother)': 'pessoa',
            r'(rol√™|role)': 'situa√ß√£o',
        }
        
        for pattern, replacement in expressions.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    async def _perform_multi_layer_analysis(self, text: str) -> Dict[str, Any]:
        """üéõÔ∏è AN√ÅLISE MULTI-CAMADAS ULTRA PROFUNDA"""
        analysis = {
            'lexical': self._analyze_lexical_layer(text),
            'syntactic': self._analyze_syntactic_layer(text),
            'semantic': self._analyze_semantic_layer(text),
            'pragmatic': self._analyze_pragmatic_layer(text)
        }
        
        # Calcular score agregado
        analysis['overall_complexity'] = sum(
            layer['complexity_score'] * processor['weight'] 
            for layer, processor in zip(analysis.values(), self.multi_layer_processors)
        )
        
        return analysis
    
    def _analyze_lexical_layer(self, text: str) -> Dict[str, Any]:
        """üìù AN√ÅLISE LEXICAL ULTRA PROFUNDA"""
        words = text.split()
        
        return {
            'word_count': len(words),
            'avg_word_length': sum(len(w) for w in words) / len(words) if words else 0,
            'complexity_score': min(len(words) * 0.1, 1.0),
            'rare_words': [w for w in words if len(w) > 8],
            'simple_words': [w for w in words if len(w) <= 4]
        }
    
    def _analyze_syntactic_layer(self, text: str) -> Dict[str, Any]:
        """üîó AN√ÅLISE SINT√ÅTICA ULTRA PROFUNDA"""
        # Detectar estruturas sint√°ticas
        has_questions = bool(re.search(r'\?', text))
        has_subordinate = bool(re.search(r'\b(que|se|quando|onde|como)\b', text))
        has_coordination = bool(re.search(r'\b(e|mas|ou|por√©m)\b', text))
        
        complexity = 0.3
        if has_questions: complexity += 0.2
        if has_subordinate: complexity += 0.3
        if has_coordination: complexity += 0.2
        
        return {
            'has_questions': has_questions,
            'has_subordinate_clauses': has_subordinate,
            'has_coordination': has_coordination,
            'complexity_score': min(complexity, 1.0)
        }
    
    def _analyze_semantic_layer(self, text: str) -> Dict[str, Any]:
        """üß† AN√ÅLISE SEM√ÇNTICA ULTRA PROFUNDA"""
        semantic_clusters = []
        cluster_scores = {}
        
        # Analisar proximidade sem√¢ntica com nossos clusters
        for cluster_name, words in self.contextual_relationship_graph.items():
            score = 0
            for word in words:
                if word in text.lower():
                    score += 1
            
            if score > 0:
                semantic_clusters.append(cluster_name)
                cluster_scores[cluster_name] = score / len(words)
        
        return {
            'semantic_clusters': semantic_clusters,
            'cluster_scores': cluster_scores,
            'complexity_score': min(len(semantic_clusters) * 0.2, 1.0),
            'semantic_density': sum(cluster_scores.values()) / max(len(cluster_scores), 1)
        }
    
    def _analyze_pragmatic_layer(self, text: str) -> Dict[str, Any]:
        """üí≠ AN√ÅLISE PRAGM√ÅTICA ULTRA PROFUNDA"""
        pragmatic_elements = {}
        
        # Detectar elementos pragm√°ticos
        for marker_type, markers in self.discourse_analyzers['pragmatic_markers'].items():
            found_markers = [m for m in markers if m in text.lower()]
            if found_markers:
                pragmatic_elements[marker_type] = found_markers
        
        return {
            'pragmatic_elements': pragmatic_elements,
            'complexity_score': min(len(pragmatic_elements) * 0.25, 1.0),
            'pragmatic_richness': len(pragmatic_elements)
        }
    
    async def _perform_semantic_analysis(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """üî¨ AN√ÅLISE SEM√ÇNTICA MEGA ULTRA AVAN√áADA"""
        semantic_analysis = {}
        
        # Calcular similaridade sem√¢ntica com padr√µes conhecidos
        for pattern_id, pattern in self.semantic_patterns.items():
            similarity = self._calculate_semantic_similarity(text, pattern)
            semantic_analysis[pattern_id] = similarity
        
        # An√°lise de vetores sem√¢nticos
        vector_analysis = self._analyze_semantic_vectors(text)
        
        return {
            'pattern_similarities': semantic_analysis,
            'vector_analysis': vector_analysis,
            'best_match': max(semantic_analysis.items(), key=lambda x: x[1]) if semantic_analysis else None,
            'semantic_confidence': max(semantic_analysis.values()) if semantic_analysis else 0.0
        }
    
    def _calculate_semantic_similarity(self, text: str, pattern: SemanticPattern) -> float:
        """üìê CALCULAR SIMILARIDADE SEM√ÇNTICA ULTRA PRECISA"""
        similarity_score = 0.0
        total_weight = 0.0
        
        # Analisar vetores sem√¢nticos
        for concept, weight in pattern.semantic_vectors.items():
            if concept in text.lower():
                similarity_score += weight
            total_weight += weight
        
        # Normalizar score
        if total_weight > 0:
            similarity_score = similarity_score / total_weight
        
        # Boost por triggers contextuais
        for trigger in pattern.context_triggers:
            if trigger in text.lower():
                similarity_score += 0.1
        
        return min(similarity_score, 1.0)
    
    def _analyze_semantic_vectors(self, text: str) -> Dict[str, float]:
        """üßÆ AN√ÅLISE DE VETORES SEM√ÇNTICOS"""
        vector_scores = {}
        
        for main_concept, related_concepts in self.brazilian_semantic_vectors.items():
            if main_concept in text.lower():
                vector_scores[main_concept] = 1.0
                
                # Adicionar conceitos relacionados
                for related, similarity in related_concepts.items():
                    if related in text.lower():
                        vector_scores[related] = similarity
        
        return vector_scores
    
    async def _perform_pragmatic_analysis(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """üé≠ AN√ÅLISE PRAGM√ÅTICA MEGA ULTRA AVAN√áADA"""
        pragmatic_inferences = {}
        
        # Aplicar regras de implicatura
        for rule_name, rule in self.pragmatic_inference_engine['implicature_rules'].items():
            if re.search(rule['pattern'], text, re.IGNORECASE):
                pragmatic_inferences[rule_name] = {
                    'inference': rule['inference'],
                    'confidence': rule['confidence']
                }
        
        # An√°lise contextual baseada na conversa anterior
        contextual_inferences = self._analyze_conversational_context(text, memory)
        
        return {
            'implicatures': pragmatic_inferences,
            'contextual_inferences': contextual_inferences,
            'pragmatic_confidence': max(
                [inf['confidence'] for inf in pragmatic_inferences.values()] + [0.0]
            )
        }
    
    def _analyze_conversational_context(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """üí¨ AN√ÅLISE DE CONTEXTO CONVERSACIONAL ULTRA PROFUNDA"""
        inferences = {}
        
        # Analisar padr√£o baseado na √∫ltima inten√ß√£o
        if memory.intent_history:
            last_intent, confidence, timestamp = memory.intent_history[-1]
            
            # Inferir follow-ups baseados na inten√ß√£o anterior
            follow_ups = self.pragmatic_inference_engine['contextual_inference']['follow_up_questions']
            if last_intent in follow_ups:
                for possible_intent in follow_ups[last_intent]:
                    inferences[f'follow_up_{possible_intent}'] = confidence * 0.7
        
        return inferences
    
    async def _extract_ultra_advanced_entities(self, text: str, semantic_analysis: Dict[str, Any]) -> List[ExtractedEntity]:
        """üéØ EXTRA√á√ÉO ULTRA AVAN√áADA DE ENTIDADES COM CONTEXTO SEM√ÇNTICO"""
        entities = []
        
        # Usar m√©todo existente como base
        base_entities = self._extract_all_entities(text)
        
        # Enriquecer com an√°lise sem√¢ntica
        for entity in base_entities:
            # Calcular peso sem√¢ntico
            semantic_weight = 1.0
            if semantic_analysis.get('vector_analysis'):
                for concept, score in semantic_analysis['vector_analysis'].items():
                    if concept in entity.value.lower():
                        semantic_weight = max(semantic_weight, score)
            
            # Adicionar alternativas baseadas em similaridade
            alternatives = self._find_entity_alternatives(entity, semantic_analysis)
            
            # Criar entidade enriquecida
            ultra_entity = ExtractedEntity(
                type=entity.type,
                value=entity.value,
                confidence=entity.confidence,
                context=entity.context,
                semantic_weight=semantic_weight,
                alternatives=alternatives,
                relationships=self._find_entity_relationships(entity, text)
            )
            
            entities.append(ultra_entity)
        
        return entities
    
    def _find_entity_alternatives(self, entity: ExtractedEntity, semantic_analysis: Dict[str, Any]) -> List[str]:
        """üîç ENCONTRAR ALTERNATIVAS SEM√ÇNTICAS PARA ENTIDADES"""
        alternatives = []
        
        if entity.type == 'valores_monetarios':
            alternatives = ['valor', 'quantia', 'dinheiro', 'pre√ßo', 'custo']
        elif entity.type == 'datas':
            alternatives = ['prazo', 'vencimento', 'data', 'dia', 'quando']
        
        return alternatives
    
    def _find_entity_relationships(self, entity: ExtractedEntity, text: str) -> Dict[str, float]:
        """üï∏Ô∏è ENCONTRAR RELACIONAMENTOS ENTRE ENTIDADES"""
        relationships = {}
        
        # Analisar proximidade com outras palavras-chave
        for cluster_name, words in self.contextual_relationship_graph.items():
            for word in words:
                if word in text.lower() and word != entity.value.lower():
                    relationships[word] = 0.8  # Score de relacionamento
        
        return relationships
    
    async def _analyze_ultra_emotion(self, text: str, memory: ConversationMemory) -> str:
        """üòä AN√ÅLISE EMOCIONAL ULTRA AVAN√áADA COM MEM√ìRIA"""
        # Usar an√°lise existente como base
        base_emotion = self._analyze_emotion(text)
        
        # Enriquecer com contexto de mem√≥ria emocional
        if memory.emotional_journey:
            # Considerar padr√£o emocional hist√≥rico
            recent_emotions = [emotion for emotion, score, timestamp in memory.emotional_journey[-3:]]
            
            # Se h√° padr√£o de frustra√ß√£o crescente
            if recent_emotions.count('frustrado') >= 2:
                if base_emotion in ['neutro', 'confuso']:
                    base_emotion = 'frustrado'  # Inferir frustra√ß√£o continuada
        
        # Detectar escalation emocional
        emotional_escalation = self._detect_emotional_escalation(text)
        if emotional_escalation:
            if base_emotion == 'frustrado':
                base_emotion = 'muito_frustrado'  # Nova categoria
            elif base_emotion == 'urgente':
                base_emotion = 'extremamente_urgente'  # Nova categoria
        
        return base_emotion
    
    def _detect_emotional_escalation(self, text: str) -> bool:
        """üìà DETECTAR ESCALATION EMOCIONAL"""
        escalation_markers = [
            r'(muito|extremamente|super|ultra).*(chateado|irritado)',
            r'(n√£o.*aguentar|n√£o.*suportar)',
            r'(absurdo|rid√≠culo|inaceit√°vel)',
            r'[!]{3,}',  # M√∫ltiplas exclama√ß√µes
            r'[?!]{2,}',  # Mistura de ? e !
        ]
        
        return any(re.search(pattern, text, re.IGNORECASE) for pattern in escalation_markers)
    
    async def _analyze_ultra_temporal_context(self, text: str, memory: ConversationMemory) -> str:
        """‚è∞ AN√ÅLISE TEMPORAL ULTRA AVAN√áADA"""
        base_temporal = self._analyze_temporal_context(text)
        
        # Enriquecer com an√°lise de urg√™ncia temporal
        urgency_indicators = {
            'imediato': ['agora', 'j√°', 'imediatamente', 'urgente'],
            'hoje': ['hoje', 'hj', 'ainda hoje'],
            'breve': ['logo', 'em breve', 'rapidinho'],
            'futuro_proximo': ['amanh√£', 'essa semana', 'uns dias'],
            'futuro_distante': ['m√™s que vem', 'ano que vem', 'mais tarde']
        }
        
        for urgency_level, indicators in urgency_indicators.items():
            if any(indicator in text.lower() for indicator in indicators):
                return f"{base_temporal}_{urgency_level}"
        
        return base_temporal
    
    async def _analyze_ultra_negation(self, text: str) -> Dict[str, Any]:
        """‚ùå AN√ÅLISE ULTRA AVAN√áADA DE NEGA√á√ÉO"""
        has_basic_negation = self._detect_negation(text)
        
        # An√°lise mais sofisticada de tipos de nega√ß√£o
        negation_types = {
            'absolute': r'\b(nunca|jamais|de jeito nenhum)\b',
            'partial': r'\b(n√£o muito|meio que n√£o|acho que n√£o)\b',
            'conditional': r'\b(n√£o se|s√≥ n√£o|a n√£o ser)\b',
            'emphatic': r'\b(de forma alguma|nem pensar|que nada)\b'
        }
        
        detected_types = []
        for neg_type, pattern in negation_types.items():
            if re.search(pattern, text, re.IGNORECASE):
                detected_types.append(neg_type)
        
        return {
            'has_negation': has_basic_negation,
            'negation_types': detected_types,
            'negation_strength': len(detected_types) / len(negation_types)
        }
    
    async def _analyze_ultra_contextual_intent(
        self, text: str, entities: List[ExtractedEntity], emotion: str, 
        temporal: str, negation: Dict, memory: ConversationMemory, 
        semantic_analysis: Dict, pragmatic_analysis: Dict
    ) -> ContextualIntent:
        """üß† AN√ÅLISE ULTRA MEGA AVAN√áADA DE INTEN√á√ÉO CONTEXTUAL"""
        
        # Usar an√°lise base existente
        base_intent_analysis = self._analyze_contextual_intent(
            text, entities, emotion, temporal, negation.get('has_negation', False), memory
        )
        
        # ENRIQUECER COM AN√ÅLISES ULTRA AVAN√áADAS
        
        # 1. Boost sem√¢ntico baseado na melhor correspond√™ncia
        if semantic_analysis.get('best_match'):
            pattern_id, similarity_score = semantic_analysis['best_match']
            if similarity_score > 0.7:
                # Aplicar boost baseado no padr√£o sem√¢ntico
                if 'fatura' in pattern_id:
                    base_intent_analysis.confidence += 0.2
                elif 'valor' in pattern_id:
                    base_intent_analysis.confidence += 0.15
        
        # 2. Boost pragm√°tico baseado em implicaturas
        pragmatic_confidence = pragmatic_analysis.get('pragmatic_confidence', 0)
        base_intent_analysis.confidence += pragmatic_confidence * 0.1
        
        # 3. Calcular similaridade sem√¢ntica com inten√ß√µes conhecidas
        semantic_similarity = self._calculate_intent_semantic_similarity(
            base_intent_analysis.intent, semantic_analysis
        )
        
        # 4. Analisar alternativas de inten√ß√£o
        alternative_intents = self._calculate_alternative_intents(
            text, semantic_analysis, pragmatic_analysis
        )
        
        # 5. Detectar clusters sem√¢nticos
        semantic_clusters = semantic_analysis.get('pattern_similarities', {}).keys()
        
        # 6. Analisar marcadores de discurso
        discourse_markers = self._extract_discourse_markers(text)
        
        # 7. Infer√™ncia pragm√°tica ultra avan√ßada
        pragmatic_inference = self._calculate_pragmatic_inference(
            base_intent_analysis, pragmatic_analysis, memory
        )
        
        # Criar inten√ß√£o contextual ultra enriquecida
        ultra_intent = ContextualIntent(
            intent=base_intent_analysis.intent,
            confidence=min(base_intent_analysis.confidence, 1.0),
            entities=entities,
            temporal_context=temporal,
            emotional_state=emotion,
            negation=negation.get('has_negation', False),
            multiple_intents=base_intent_analysis.multiple_intents,
            
            # CAMPOS ULTRA AVAN√áADOS
            semantic_similarity=semantic_similarity,
            contextual_coherence=0.0,  # Ser√° calculado depois
            linguistic_complexity=semantic_analysis.get('semantic_confidence', 0),
            intent_certainty=0.0,  # Ser√° calculado depois
            alternative_intents=alternative_intents,
            semantic_clusters=list(semantic_clusters),
            discourse_markers=discourse_markers,
            pragmatic_inference=pragmatic_inference
        )
        
        return ultra_intent
    
    def _calculate_intent_semantic_similarity(self, intent: IntentType, semantic_analysis: Dict) -> float:
        """üìê CALCULAR SIMILARIDADE SEM√ÇNTICA DA INTEN√á√ÉO"""
        intent_key = intent.value
        similarity_matrix = self.intent_similarity_matrix
        
        if intent_key in similarity_matrix:
            # Calcular m√©dia das similaridades com outras inten√ß√µes detectadas
            similarities = []
            for related_intent, similarity in similarity_matrix[intent_key].items():
                if any(related_intent in cluster for cluster in semantic_analysis.get('pattern_similarities', {})):
                    similarities.append(similarity)
            
            return sum(similarities) / len(similarities) if similarities else 0.5
        
        return 0.5  # Default
    
    def _calculate_alternative_intents(self, text: str, semantic_analysis: Dict, pragmatic_analysis: Dict) -> List[Tuple[IntentType, float]]:
        """üéØ CALCULAR INTEN√á√ïES ALTERNATIVAS"""
        alternatives = []
        
        # Baseado em an√°lise sem√¢ntica
        for pattern_id, similarity in semantic_analysis.get('pattern_similarities', {}).items():
            if similarity > 0.5:
                if 'fatura' in pattern_id:
                    alternatives.append((IntentType.FATURA_SOLICITAR, similarity))
                elif 'valor' in pattern_id:
                    alternatives.append((IntentType.FATURA_VALOR, similarity))
                elif 'negociacao' in pattern_id:
                    alternatives.append((IntentType.NEGOCIACAO_PARCELAMENTO, similarity))
        
        # Remover duplicatas e ordenar por confian√ßa
        alternatives = list(set(alternatives))
        alternatives.sort(key=lambda x: x[1], reverse=True)
        
        return alternatives[:3]  # Top 3 alternativas
    
    def _extract_discourse_markers(self, text: str) -> List[str]:
        """üí¨ EXTRAIR MARCADORES DE DISCURSO"""
        markers = []
        
        for marker_type, marker_list in self.discourse_analyzers['discourse_markers'].items():
            for marker in marker_list:
                if marker in text.lower():
                    markers.append(f"{marker_type}:{marker}")
        
        return markers
    
    def _calculate_pragmatic_inference(self, intent: ContextualIntent, pragmatic_analysis: Dict, memory: ConversationMemory) -> Dict[str, float]:
        """üé≠ CALCULAR INFER√äNCIA PRAGM√ÅTICA"""
        inferences = {}
        
        # Infer√™ncias baseadas em implicaturas
        for implicature_name, implicature_data in pragmatic_analysis.get('implicatures', {}).items():
            inferences[implicature_name] = implicature_data['confidence']
        
        # Infer√™ncias contextuais
        contextual_infs = pragmatic_analysis.get('contextual_inferences', {})
        inferences.update(contextual_infs)
        
        return inferences
    
    async def _analyze_contextual_coherence(self, intent: ContextualIntent, memory: ConversationMemory) -> float:
        """üîó ANALISAR COER√äNCIA CONTEXTUAL"""
        if not memory.intent_history:
            return 0.8  # Primeira mensagem tem coer√™ncia neutra
        
        # Pegar √∫ltimas 3 inten√ß√µes
        recent_intents = [intent_data[0] for intent_data in memory.intent_history[-3:]]
        current_intent = intent.intent.value
        
        # Calcular coer√™ncia baseada na matriz de similaridade
        coherence_scores = []
        
        for past_intent in recent_intents:
            if past_intent in self.intent_similarity_matrix:
                if current_intent in self.intent_similarity_matrix[past_intent]:
                    coherence_scores.append(self.intent_similarity_matrix[past_intent][current_intent])
                else:
                    coherence_scores.append(0.3)  # Baixa coer√™ncia para inten√ß√µes n√£o relacionadas
        
        return sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0.5
    
    async def _calculate_intent_certainty(self, intent: ContextualIntent, linguistic_analysis: Dict) -> float:
        """‚úÖ CALCULAR CERTEZA DA INTEN√á√ÉO"""
        certainty_factors = []
        
        # Fator 1: Confian√ßa base da inten√ß√£o
        certainty_factors.append(intent.confidence)
        
        # Fator 2: Similaridade sem√¢ntica
        certainty_factors.append(intent.semantic_similarity)
        
        # Fator 3: Coer√™ncia contextual
        certainty_factors.append(intent.contextual_coherence)
        
        # Fator 4: Complexidade lingu√≠stica (menos complexo = mais certo)
        linguistic_certainty = 1.0 - linguistic_analysis.get('overall_complexity', 0.5)
        certainty_factors.append(linguistic_certainty)
        
        # Fator 5: Presen√ßa de entidades relevantes
        entity_certainty = min(len(intent.entities) * 0.2, 1.0)
        certainty_factors.append(entity_certainty)
        
        # Calcular m√©dia ponderada
        weights = [0.3, 0.2, 0.2, 0.15, 0.15]  # Soma = 1.0
        weighted_certainty = sum(factor * weight for factor, weight in zip(certainty_factors, weights))
        
        return min(weighted_certainty, 1.0)
    
    async def _update_ultra_conversation_memory(self, phone: str, intent: ContextualIntent, text: str, linguistic_analysis: Dict):
        """üß† ATUALIZAR MEM√ìRIA ULTRA AVAN√áADA"""
        memory = self.conversation_memories[phone]
        
        # Atualizar hist√≥rico de inten√ß√µes
        memory.intent_history.append((
            intent.intent.value, 
            intent.confidence, 
            datetime.now()
        ))
        
        # Atualizar jornada emocional
        memory.emotional_journey.append((
            intent.emotional_state,
            intent.confidence,
            datetime.now()
        ))
        
        # Atualizar padr√µes de conversa√ß√£o
        memory.conversation_patterns.append(text[:100])  # Primeiros 100 chars
        
        # Detectar mudan√ßas de contexto
        if len(memory.intent_history) > 1:
            last_intent = memory.intent_history[-2][0]
            if intent.intent.value != last_intent:
                if intent.contextual_coherence < 0.4:  # Mudan√ßa abrupta
                    memory.context_switches.append(datetime.now())
        
        # Atualizar dados de aprendizado
        memory.learning_data['total_messages'] = memory.learning_data.get('total_messages', 0) + 1
        memory.learning_data['avg_confidence'] = (
            memory.learning_data.get('avg_confidence', 0.5) + intent.confidence
        ) / 2
        
        # Manter apenas √∫ltimos 50 registros de cada tipo
        memory.intent_history = memory.intent_history[-50:]
        memory.emotional_journey = memory.emotional_journey[-50:]
        memory.conversation_patterns = memory.conversation_patterns[-50:]
        memory.context_switches = memory.context_switches[-20:]
    
    async def _learn_from_interaction(self, phone: str, intent: ContextualIntent, semantic_analysis: Dict):
        """üéì APRENDER A PARTIR DA INTERA√á√ÉO"""
        # Armazenar padr√µes bem-sucedidos para aprendizado futuro
        if intent.confidence > 0.8:
            pattern_key = f"{intent.intent.value}_{intent.emotional_state}"
            
            if pattern_key not in self.pattern_learning_db:
                self.pattern_learning_db[pattern_key] = []
            
            # Armazenar caracter√≠sticas da mensagem bem entendida
            learning_pattern = {
                'semantic_clusters': intent.semantic_clusters,
                'entities_count': len(intent.entities),
                'discourse_markers': intent.discourse_markers,
                'confidence': intent.confidence,
                'timestamp': datetime.now()
            }
            
            self.pattern_learning_db[pattern_key].append(learning_pattern)
            
            # Manter apenas √∫ltimos 20 padr√µes por tipo
            self.pattern_learning_db[pattern_key] = self.pattern_learning_db[pattern_key][-20:]
    
    async def _generate_ultra_contextual_response(
        self, phone: str, intent: ContextualIntent, entities: List[ExtractedEntity], 
        memory: ConversationMemory, semantic_analysis: Dict
    ) -> Dict[str, Any]:
        """üé≠ GERA√á√ÉO ULTRA INTELIGENTE DE RESPOSTA N√çVEL CHATGPT"""
        
        # Usar gerador existente como base
        base_response = await self._generate_contextual_response(phone, intent, entities, {})
        
        # ENRIQUECER COM INTELIG√äNCIA ULTRA AVAN√áADA
        
        # 1. Personaliza√ß√£o baseada em mem√≥ria
        personalization = self._generate_personalized_elements(memory, intent)
        
        # 2. Adapta√ß√£o baseada em certeza
        certainty_adaptation = self._adapt_response_for_certainty(intent.intent_certainty)
        
        # 3. Contextualiza√ß√£o sem√¢ntica
        semantic_context = self._add_semantic_context(semantic_analysis, intent)
        
        # 4. Resposta din√¢mica baseada em padr√µes aprendidos
        learned_enhancements = self._apply_learned_patterns(intent, memory)
        
        # Gerar resposta ultra contextualizada
        ultra_response_text = self._compose_ultra_response(
            base_response['text'], personalization, certainty_adaptation, 
            semantic_context, learned_enhancements, intent
        )
        
        return {
            'text': ultra_response_text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'entities_detected': len(entities),
            'emotional_state': intent.emotional_state,
            'multiple_intents': len(intent.multiple_intents),
            'context_enhanced': True,
            'response_type': 'ultra_mega_contextual',
            
            # NOVOS CAMPOS ULTRA AVAN√áADOS
            'semantic_similarity': intent.semantic_similarity,
            'contextual_coherence': intent.contextual_coherence,
            'intent_certainty': intent.intent_certainty,
            'personalization_level': len(personalization),
            'semantic_clusters': intent.semantic_clusters,
            'discourse_markers': intent.discourse_markers,
            'ultra_enhanced': True
        }
    
    def _generate_personalized_elements(self, memory: ConversationMemory, intent: ContextualIntent) -> Dict[str, str]:
        """üë§ GERAR ELEMENTOS PERSONALIZADOS"""
        personalization = {}
        
        # Baseado em padr√£o emocional
        if memory.emotional_journey:
            recent_emotions = [emotion for emotion, _, _ in memory.emotional_journey[-3:]]
            if recent_emotions.count('frustrado') >= 2:
                personalization['empathy'] = "Eu vejo que voc√™ est√° passando por uma situa√ß√£o chata"
            elif recent_emotions.count('urgente') >= 2:
                personalization['urgency_ack'] = "Entendo que isso √© urgente para voc√™"
        
        # Baseado em hist√≥rico de inten√ß√µes
        if memory.intent_history:
            common_intents = Counter([intent for intent, _, _ in memory.intent_history])
            most_common = common_intents.most_common(1)[0][0]
            if most_common == 'fatura_solicitar':
                personalization['context'] = "Como sempre, vou buscar sua fatura"
        
        return personalization
    
    def _adapt_response_for_certainty(self, certainty: float) -> Dict[str, str]:
        """‚úÖ ADAPTAR RESPOSTA BASEADA NA CERTEZA"""
        if certainty > 0.9:
            return {'confidence_marker': '**CERTEZA ABSOLUTA!**', 'action': 'Vou resolver isso AGORA!'}
        elif certainty > 0.7:
            return {'confidence_marker': '**ENTENDI PERFEITAMENTE!**', 'action': 'Vou cuidar disso!'}
        elif certainty > 0.5:
            return {'confidence_marker': '**ACHO QUE ENTENDI!**', 'action': 'Deixe-me confirmar...'}
        else:
            return {'confidence_marker': '**HMMMM...**', 'action': 'Me explica melhor?'}
    
    def _add_semantic_context(self, semantic_analysis: Dict, intent: ContextualIntent) -> Dict[str, str]:
        """üß† ADICIONAR CONTEXTO SEM√ÇNTICO"""
        context = {}
        
        if semantic_analysis.get('best_match'):
            pattern_id, score = semantic_analysis['best_match']
            if score > 0.8:
                context['semantic_confidence'] = f"Detectei {int(score*100)}% de certeza"
        
        return context
    
    def _apply_learned_patterns(self, intent: ContextualIntent, memory: ConversationMemory) -> Dict[str, str]:
        """üéì APLICAR PADR√ïES APRENDIDOS"""
        enhancements = {}
        
        pattern_key = f"{intent.intent.value}_{intent.emotional_state}"
        if pattern_key in self.pattern_learning_db:
            patterns = self.pattern_learning_db[pattern_key]
            if patterns:
                # Aplicar insights dos padr√µes aprendidos
                avg_confidence = sum(p['confidence'] for p in patterns) / len(patterns)
                if avg_confidence > 0.8:
                    enhancements['learned_boost'] = "Baseado no que aprendi com voc√™"
        
        return enhancements
    
    def _compose_ultra_response(
        self, base_text: str, personalization: Dict, certainty: Dict, 
        semantic: Dict, learned: Dict, intent: ContextualIntent
    ) -> str:
        """üé≠ COMPOR RESPOSTA ULTRA AVAN√áADA"""
        
        # Come√ßar com texto base
        response_parts = [base_text]
        
        # Adicionar personaliza√ß√£o
        if personalization.get('empathy'):
            response_parts.insert(0, personalization['empathy'] + ".")
        
        # Adicionar marcador de confian√ßa
        if certainty.get('confidence_marker'):
            response_parts[0] = response_parts[0].replace(
                response_parts[0].split()[0], 
                certainty['confidence_marker']
            )
        
        # Adicionar contexto sem√¢ntico se alta confian√ßa
        if semantic.get('semantic_confidence'):
            response_parts.append(f"\n\nüéØ {semantic['semantic_confidence']} no que voc√™ quis dizer!")
        
        # Adicionar insights aprendidos
        if learned.get('learned_boost'):
            response_parts.append(f"\n\nüß† {learned['learned_boost']}, sei exatamente o que fazer!")
        
        return " ".join(response_parts)
    
    async def _ultra_intelligent_fallback(self, phone: str, text: str, error: Exception) -> Dict[str, Any]:
        """üõ°Ô∏è FALLBACK ULTRA INTELIGENTE MULTI-CAMADAS"""
        
        logger.error(f"üöÄ Ativando fallback ultra inteligente para: {text[:50]}... | Erro: {error}")
        
        # Tentar fallbacks em cascata
        for fallback_level in self.intelligent_fallback_system['fallback_levels']:
            try:
                if fallback_level['name'] == 'semantic_similarity':
                    return await self._fallback_semantic_similarity(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'keyword_extraction':
                    return await self._fallback_keyword_extraction(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'pattern_matching':
                    return await self._fallback_pattern_matching(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'conversational_context':
                    return await self._fallback_conversational_context(phone, text, fallback_level['threshold'])
                elif fallback_level['name'] == 'intelligent_guess':
                    return await self._fallback_intelligent_guess(text, fallback_level['threshold'])
                    
            except Exception as fallback_error:
                logger.warning(f"Fallback n√≠vel {fallback_level['level']} falhou: {fallback_error}")
                continue
        
        # Fallback final de emerg√™ncia
        return {
            'text': "ü§î **NOSSA!** Essa foi dif√≠cil at√© para mim! Pode tentar falar de um jeito mais simples? Tipo: 'quero minha conta' ou 'quanto devo'?",
            'intent': 'emergency_fallback',
            'confidence': 0.1,
            'fallback_level': 'emergency',
            'ultra_enhanced': True
        }
    
    async def _fallback_semantic_similarity(self, text: str, threshold: float) -> Dict[str, Any]:
        """üîç FALLBACK POR SIMILARIDADE SEM√ÇNTICA"""
        # Tentar encontrar padr√£o sem√¢ntico mais pr√≥ximo
        best_match = None
        best_score = 0.0
        
        for pattern_id, pattern in self.semantic_patterns.items():
            score = self._calculate_semantic_similarity(text, pattern)
            if score > best_score and score > threshold:
                best_match = pattern_id
                best_score = score
        
        if best_match:
            intent_mapping = {
                'fatura_semantic': 'fatura_solicitar',
                'valor_semantic': 'fatura_valor',
                'tempo_semantic': 'fatura_vencimento',
                'negociacao_semantic': 'negociacao_parcelamento'
            }
            
            inferred_intent = intent_mapping.get(best_match, 'fatura_solicitar')
            
            return {
                'text': f"üéØ **ENTENDI PELO CONTEXTO!** Voc√™ quer algo relacionado a {inferred_intent.replace('_', ' ')}. Vou ajudar!",
                'intent': inferred_intent,
                'confidence': best_score,
                'fallback_level': 'semantic_similarity',
                'ultra_enhanced': True
            }
        
        raise Exception("Similaridade sem√¢ntica insuficiente")
    
    async def _fallback_keyword_extraction(self, text: str, threshold: float) -> Dict[str, Any]:
        """üîë FALLBACK POR EXTRA√á√ÉO DE PALAVRAS-CHAVE"""
        keywords = {
            'fatura': ['conta', 'boleto', 'fatura', 'segunda', 'via', 'papel'],
            'valor': ['quanto', 'valor', 'devo', 'pagar', 'pre√ßo', 'dinheiro'],
            'vencimento': ['quando', 'vence', 'prazo', 'data', 'at√©'],
            'negociacao': ['parcelar', 'acordo', 'desconto', 'negociar', 'facilitar']
        }
        
        scores = {}
        for intent, intent_keywords in keywords.items():
            score = sum(1 for keyword in intent_keywords if keyword in text.lower())
            if score > 0:
                scores[intent] = score / len(intent_keywords)
        
        if scores:
            best_intent = max(scores.items(), key=lambda x: x[1])
            if best_intent[1] > threshold:
                return {
                    'text': f"üîç **CAPTEI!** Pelas palavras-chave, voc√™ quer {best_intent[0]}. √â isso mesmo?",
                    'intent': best_intent[0],
                    'confidence': best_intent[1],
                    'fallback_level': 'keyword_extraction',
                    'ultra_enhanced': True
                }
        
        raise Exception("Palavras-chave insuficientes")
    
    async def _fallback_pattern_matching(self, text: str, threshold: float) -> Dict[str, Any]:
        """üß© FALLBACK POR CORRESPOND√äNCIA DE PADR√ïES"""
        # Padr√µes de emerg√™ncia muito b√°sicos
        emergency_patterns = [
            (r'\b(conta|boleto|fatura)\b', 'fatura_solicitar', 0.7),
            (r'\b(quanto|valor)\b', 'fatura_valor', 0.6),
            (r'\b(quando|vence|prazo)\b', 'fatura_vencimento', 0.6),
            (r'\b(paguei|pago)\b', 'pagamento_confirmacao', 0.5),
            (r'\b(parcelar|acordo)\b', 'negociacao_parcelamento', 0.5),
        ]
        
        for pattern, intent, confidence in emergency_patterns:
            if re.search(pattern, text, re.IGNORECASE) and confidence > threshold:
                return {
                    'text': f"üß© **CONSEGUI ENTENDER!** Pelo padr√£o, voc√™ quer {intent.replace('_', ' ')}!",
                    'intent': intent,
                    'confidence': confidence,
                    'fallback_level': 'pattern_matching',
                    'ultra_enhanced': True
                }
        
        raise Exception("Nenhum padr√£o corresponde")
    
    async def _fallback_conversational_context(self, phone: str, text: str, threshold: float) -> Dict[str, Any]:
        """üí≠ FALLBACK POR CONTEXTO CONVERSACIONAL"""
        if phone in self.conversation_memories:
            memory = self.conversation_memories[phone]
            if memory.intent_history:
                # Assumir que √© follow-up da √∫ltima inten√ß√£o
                last_intent, last_confidence, _ = memory.intent_history[-1]
                
                if last_confidence > threshold:
                    return {
                        'text': f"üí≠ **PELO CONTEXTO!** Voc√™ ainda est√° falando sobre {last_intent.replace('_', ' ')}, n√©?",
                        'intent': last_intent,
                        'confidence': last_confidence * 0.8,
                        'fallback_level': 'conversational_context',
                        'ultra_enhanced': True
                    }
        
        raise Exception("Contexto conversacional insuficiente")
    
    async def _fallback_intelligent_guess(self, text: str, threshold: float) -> Dict[str, Any]:
        """üé≤ FALLBACK POR SUPOSI√á√ÉO INTELIGENTE"""
        # Se chegou at√© aqui, fazer uma suposi√ß√£o educada baseada no contexto de cobran√ßa
        text_length = len(text.split())
        
        if text_length <= 3:
            # Texto muito curto - provavelmente quer fatura
            guess_intent = 'fatura_solicitar'
            guess_confidence = 0.4
        elif '?' in text:
            # Tem pergunta - provavelmente quer informa√ß√£o (valor ou vencimento)
            guess_intent = 'fatura_valor'
            guess_confidence = 0.3
        else:
            # Default para solicita√ß√£o de fatura
            guess_intent = 'fatura_solicitar'
            guess_confidence = 0.2
        
        if guess_confidence > threshold:
            return {
                'text': f"üé≤ **VAMOS TENTAR!** Pelo contexto geral, acho que voc√™ quer {guess_intent.replace('_', ' ')}. Se n√£o for isso, me fala 'n√£o' que eu entendo outra coisa!",
                'intent': guess_intent,
                'confidence': guess_confidence,
                'fallback_level': 'intelligent_guess',
                'ultra_enhanced': True,
                'requires_confirmation': True
            }
        
        raise Exception("Imposs√≠vel fazer suposi√ß√£o v√°lida") 